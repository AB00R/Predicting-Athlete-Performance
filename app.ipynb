{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b959adb8",
   "metadata": {},
   "source": [
    "# Athlete Performance Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00326be",
   "metadata": {},
   "source": [
    "An AI-driven system designed to predict athlete performance and assess the risk of injury. This system leverages historical performance data, physiological metrics, and contextual factors to provide actionable insights. \n",
    "\n",
    "Key features of the system include:\n",
    "- **Performance Prediction**: Forecasts athlete performance based on training schedules, fatigue levels, and other metrics.\n",
    "- **Injury Risk Assessment**: Identifies athletes at risk of injury by analyzing physiological and contextual data.\n",
    "- **Search Techniques**: Implements advanced search algorithms such as Greedy Search, A* Search, and Uninformed Search to optimize training plans.\n",
    "- **Constraint Satisfaction Problems (CSP)**: Ensures training plans adhere to constraints like fatigue thresholds and risk limits.\n",
    "- **Optimization Algorithms**: Utilizes techniques like Genetic Algorithms to explore and identify optimal training schedules.\n",
    "\n",
    "This system aims to enhance decision-making in athlete training and performance management by combining predictive analytics with optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c286847",
   "metadata": {},
   "source": [
    "## 0.0 Data Collection & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────\n",
    "RawData     = \"data/subjective.zip\"\n",
    "cleanedData   = \"data/soccer_data_cleaned.csv\"\n",
    "os.makedirs(os.path.dirname(cleanedData) or \".\", exist_ok=True)\n",
    "\n",
    "# ─── 1) EXTRACT & LOAD ─────────────────────────────────────────────────────\n",
    "with zipfile.ZipFile(RawData, \"r\") as z:\n",
    "    # 1a) daily load\n",
    "    with z.open(\"subjective/training-load/daily_load.csv\") as f:\n",
    "        df_load = pd.read_csv(f)\n",
    "    df_load = (\n",
    "        df_load.rename(columns={\"Date\": \"date\"})\n",
    "               .assign(date=lambda d: pd.to_datetime(d[\"date\"], dayfirst=True))\n",
    "               .melt(id_vars=\"date\", var_name=\"player_id\", value_name=\"load\")\n",
    "    )\n",
    "\n",
    "    # 1b) session-RPE JSON\n",
    "    with z.open(\"subjective/training-load/session.json\") as f:\n",
    "        sess = json.load(f)\n",
    "    rows = []\n",
    "    for pid, sess_list in sess.items():\n",
    "        for s in sess_list:\n",
    "            rows.append({\n",
    "                \"player_id\":     pid,\n",
    "                \"date\":          pd.to_datetime(s[\"date\"], dayfirst=True),\n",
    "                \"total_duration\": s[\"duration\"]\n",
    "            })\n",
    "    df_sess = (pd.DataFrame(rows)\n",
    "                 .groupby([\"player_id\", \"date\"], as_index=False)\n",
    "                 .agg(total_duration=(\"total_duration\", \"sum\")))\n",
    "\n",
    "    # 1c) game-performance\n",
    "    with z.open(\"subjective/game-performance/game-performance.csv\") as f:\n",
    "        df_game = pd.read_csv(f)\n",
    "    df_game = (\n",
    "        df_game.assign(\n",
    "            date=pd.to_datetime(df_game[\"timestamp\"], format=\"%d.%m.%Y\")\n",
    "        )\n",
    "        .rename(columns={\"player_name\": \"player_id\"})\n",
    "    )\n",
    "    df_game[\"performance_metric\"] = (\n",
    "        df_game[\"offensive_performance\"] + df_game[\"defensive_performance\"]\n",
    "    ) / 2\n",
    "\n",
    "    # 1d) injury\n",
    "    with z.open(\"subjective/injury/injury.csv\") as f:\n",
    "        df_inj = pd.read_csv(f)\n",
    "    df_inj = (\n",
    "        df_inj.assign(\n",
    "            date=pd.to_datetime(df_inj[\"timestamp\"], format=\"%d.%m.%Y\"),\n",
    "            injury_flag=1\n",
    "        )\n",
    "        .rename(columns={\"player_name\": \"player_id\"})\n",
    "        [[\"player_id\", \"date\", \"injury_flag\"]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    # 1e) wellness\n",
    "    def load_well(path, col):\n",
    "        with zipfile.ZipFile(RawData) as zz:\n",
    "            with zz.open(path) as ff:\n",
    "                w = pd.read_csv(ff)\n",
    "        return (\n",
    "            w.rename(columns={w.columns[0]: \"date\"})\n",
    "             .assign(date=lambda d: pd.to_datetime(d[\"date\"], dayfirst=True))\n",
    "             .melt(id_vars=\"date\", var_name=\"player_id\", value_name=col)\n",
    "        )\n",
    "\n",
    "    wellness_map = {\n",
    "        \"subjective/wellness/fatigue.csv\":        \"fatigue_post\",\n",
    "        \"subjective/wellness/sleep_duration.csv\": \"sleep_duration\",\n",
    "        \"subjective/wellness/sleep_quality.csv\":  \"sleep_quality\",\n",
    "        \"subjective/wellness/stress.csv\":         \"stress\",\n",
    "    }\n",
    "\n",
    "# ─── 2) MERGE ALL ───────────────────────────────────────────────────────────\n",
    "# start with load\n",
    "df = df_load\n",
    "\n",
    "# merge game perf & raw metrics\n",
    "df = df.merge(\n",
    "    df_game[[\"player_id\",\"date\",\"performance_metric\",\"offensive_performance\",\"defensive_performance\"]],\n",
    "    on=[\"player_id\",\"date\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# merge injury\n",
    "df = df.merge(df_inj, on=[\"player_id\",\"date\"], how=\"left\")\n",
    "df[\"injury_flag\"] = df[\"injury_flag\"].fillna(0).astype(int)\n",
    "\n",
    "# merge wellness\n",
    "for path, col in wellness_map.items():\n",
    "    df = df.merge(load_well(path, col), on=[\"player_id\",\"date\"], how=\"left\")\n",
    "\n",
    "# merge session durations\n",
    "df = df.merge(df_sess, on=[\"player_id\",\"date\"], how=\"left\")\n",
    "df[\"total_duration\"] = df[\"total_duration\"].fillna(0)\n",
    "\n",
    "# ─── 2b) FILL SPECIFIC MISSING WELLNESS VALUES ─────────────────────────────\n",
    "fill_with_median = [\"sleep_duration\", \"sleep_quality\", \"stress\"]\n",
    "for col in fill_with_median:\n",
    "    if col in df.columns:\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "\n",
    "# ─── 3) BASIC LABELS & COUNTS ──────────────────────────────────────────────\n",
    "# classify action into five categories\n",
    "loads = df.loc[df.load > 0, \"load\"]\n",
    "low_th, med_th = loads.quantile([0.33, 0.66])\n",
    "\n",
    "def act(r):\n",
    "    if pd.notna(r.performance_metric):\n",
    "        return \"Game\"\n",
    "    if r.load == 0:\n",
    "        return \"Rest\"\n",
    "    if r.load < low_th:\n",
    "        return \"Train_Low\"\n",
    "    if r.load < med_th:\n",
    "        return \"Train_Med\"\n",
    "    return \"Train_High\"\n",
    "\n",
    "df[\"action\"] = df.apply(act, axis=1)\n",
    "df[\"player_num\"] = df[\"player_id\"].astype(\"category\").cat.codes + 1\n",
    "df = df.sort_values([\"player_num\",\"date\"]).reset_index(drop=True)\n",
    "df[\"injury_count\"] = df.groupby(\"player_num\")[\"injury_flag\"].cumsum()\n",
    "\n",
    "# ─── 4) TIME-BASED FEATURES ───────────────────────────────────────────────\n",
    "well_cols = list(wellness_map.values())\n",
    "roll_cols = [\"load\"] + well_cols\n",
    "\n",
    "# rolling 7d\n",
    "for c in roll_cols:\n",
    "    df[f\"{c}_rolling_7\"] = (\n",
    "        df.groupby(\"player_num\")[c]\n",
    "          .rolling(window=7, min_periods=1).mean()\n",
    "          .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# 1-day lags\n",
    "df[\"load_lag_1\"]        = df.groupby(\"player_num\")[\"load\"].shift(1)\n",
    "df[\"injury_flag_lag_1\"] = df.groupby(\"player_num\")[\"injury_flag\"].shift(1)\n",
    "df[\"perf_lag_1\"]        = df.groupby(\"player_num\")[\"performance_metric\"].shift(1)\n",
    "\n",
    "# days since last game\n",
    "df[\"last_game\"] = df[\"date\"].where(df[\"action\"]==\"Game\")\n",
    "df[\"last_game\"] = df.groupby(\"player_num\")[\"last_game\"].ffill()\n",
    "df[\"days_since_game\"] = (df[\"date\"] - df[\"last_game\"]).dt.days.fillna(-1).astype(int)\n",
    "df.drop(columns=[\"last_game\"], inplace=True)\n",
    "\n",
    "# rest-day flag\n",
    "df[\"is_rest_day\"] = (df[\"action\"]==\"Rest\").astype(int)\n",
    "\n",
    "# ─── 5) FILTER LONG REST BLOCKS (>90d) ────────────────────────────────────\n",
    "def remove_long(df, thresh=90):\n",
    "    def f(g):\n",
    "        g = g.copy()\n",
    "        g[\"is_r\"] = g[\"action\"].eq(\"Rest\")\n",
    "        g[\"grp\"]  = (~g[\"is_r\"]).cumsum()\n",
    "        g[\"blk\"]  = g.groupby(\"grp\")[\"is_r\"].transform(\"sum\")\n",
    "        g[\"drop\"] = g[\"is_r\"] & (g[\"blk\"]>thresh)\n",
    "        return g.loc[~g[\"drop\"], g.columns.difference([\"is_r\",\"grp\",\"blk\",\"drop\"])]\n",
    "    return df.groupby(\"player_num\", group_keys=False ).apply(f)\n",
    "\n",
    "df = remove_long(df, thresh=90)\n",
    "\n",
    "# ─── 6) FINALIZE & SAVE ───────────────────────────────────────────────────\n",
    "cols = [\n",
    "    \"player_num\",\"player_id\",\"date\",\"action\",\"load\",\n",
    "    \"performance_metric\",\"offensive_performance\",\"defensive_performance\",\n",
    "    \"injury_flag\",\"injury_count\",\"is_rest_day\",\"days_since_game\",\n",
    "    \"total_duration\"\n",
    "] + well_cols + [f\"{c}_rolling_7\" for c in roll_cols] + [\n",
    "    \"load_lag_1\",\"injury_flag_lag_1\",\"perf_lag_1\"\n",
    "]\n",
    "\n",
    "df[cols].to_csv(cleanedData, index=False)\n",
    "print(f\"data saved to {cleanedData}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5139bae",
   "metadata": {},
   "source": [
    "## 0.1 Train performance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ─── 1) LOAD & PREPARE DATA ──────────────────────────────────────────────────\n",
    "df = pd.read_csv(\"data/soccer_data_cleaned.csv\", parse_dates=[\"date\"])\n",
    "df = df.sort_values([\"player_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# map action → intensity\n",
    "action_intensity_map = {\n",
    "    \"Train_Low\":  0.3,\n",
    "    \"Train_Med\":  0.6,\n",
    "    \"Train_High\": 0.9\n",
    "}\n",
    "df[\"action_intensity\"] = df[\"action\"].map(action_intensity_map).fillna(0.0)\n",
    "\n",
    "# flag game days where performance_metric is non-null\n",
    "df = df[df[\"performance_metric\"].notna()]\n",
    "\n",
    "# ─── 2) COMPUTE TARGET: ΔP ─────────────────────────────────────────────────────\n",
    "# lag performance within each player\n",
    "df[\"performance_lag\"] = (\n",
    "    df.groupby(\"player_id\")[\"performance_metric\"]\n",
    "      .shift(1)\n",
    ")\n",
    "# drop first game-day per player (no lag)\n",
    "df[\"delta_p\"] = df[\"performance_metric\"] - df[\"performance_lag\"]\n",
    "df = df.dropna(subset=[\"delta_p\"])  # only rows with a valid delta\n",
    "\n",
    "# ─── 3) DEFINE FEATURES ──────────────────────────────────────────────────────\n",
    "FEATURES_P = [\n",
    "    \"load\",\n",
    "    \"action_intensity\",\n",
    "    \"fatigue_post\",\n",
    "    \"sleep_duration\",\n",
    "    \"sleep_quality\",\n",
    "    \"stress\",\n",
    "    \"is_rest_day\",\n",
    "    \"load_rolling_7\",\n",
    "    \"fatigue_post_rolling_7\",\n",
    "    \"sleep_duration_rolling_7\",\n",
    "    \"sleep_quality_rolling_7\",\n",
    "    \"stress_rolling_7\",\n",
    "    \"total_duration\",\n",
    "    \"load_lag_1\",\n",
    "]\n",
    "\n",
    "# ensure all those exist\n",
    "missing = set(FEATURES_P) - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns in data for ΔP features: {missing}\")\n",
    "\n",
    "# drop any rows with NaN in our chosen features\n",
    "df_p = df.dropna(subset=FEATURES_P)\n",
    "\n",
    "X = df_p[FEATURES_P]\n",
    "y = df_p[\"delta_p\"]\n",
    "\n",
    "# ─── 4) SPLIT & TRAIN ─────────────────────────────────────────────────────────\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_p = RandomForestRegressor(\n",
    "    n_estimators=250,\n",
    "    max_depth=8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_p.fit(X_train, y_train)\n",
    "\n",
    "# ─── 5) EVALUATE ──────────────────────────────────────────────────────────────\n",
    "y_pred = model_p.predict(X_val)\n",
    "print(f\"ΔP samples = {len(y):6d}\")\n",
    "print(f\"MSE       = {mean_squared_error(y_val, y_pred):.4f}\")\n",
    "print(f\"R²        = {r2_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "# ─── 6) SAVE MODEL ────────────────────────────────────────────────────────────\n",
    "joblib.dump(model_p, \"predictingModels/delta_p_model.pkl\")\n",
    "print(\"ΔP model trained and saved in predictingModels/delta_p_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025be7a",
   "metadata": {},
   "source": [
    "## 0.2 Train fatigue model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba860044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ─── 1) LOAD & PREPARE ──────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(\"data/soccer_data_cleaned.csv\", parse_dates=[\"date\"])\n",
    "df = df.sort_values([\"player_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Recompute a 7‑day rolling average on fatigue_post\n",
    "df[\"fatigue_post_rolling_7\"] = (\n",
    "    df\n",
    "    .groupby(\"player_id\")[\"fatigue_post\"]\n",
    "    .transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# ─── 2) INJECT INTENSITY & COMPUTE ΔF ───────────────────────────────────────────\n",
    "# Map action → intensity\n",
    "action_intensity_map = {\n",
    "    \"Train_Low\":  0.3,\n",
    "    \"Train_Med\":  0.6,\n",
    "    \"Train_High\": 0.9,\n",
    "}\n",
    "df[\"action_intensity\"] = df[\"action\"].map(action_intensity_map).fillna(0.0)\n",
    "\n",
    "# Compute next‑day delta_f\n",
    "df[\"delta_f\"] = (\n",
    "    df.groupby(\"player_id\")[\"fatigue_post\"].shift(-1)\n",
    "    - df[\"fatigue_post\"]\n",
    ")\n",
    "\n",
    "# ─── 3) FEATURES & FILTER ───────────────────────────────────────────────────────\n",
    "FEATURES_F = [\n",
    "    \"load\",\n",
    "    \"action_intensity\",\n",
    "    \"fatigue_post\",      \n",
    "    \"sleep_duration\",\n",
    "    \"sleep_quality\",\n",
    "    \"stress\",\n",
    "    \"is_rest_day\",\n",
    "    \"load_rolling_7\",\n",
    "    \"fatigue_post_rolling_7\", \n",
    "    \"stress_rolling_7\",\n",
    "    \"sleep_duration_rolling_7\",\n",
    "    \"sleep_quality_rolling_7\",\n",
    "    \"total_duration\",\n",
    "    \"load_lag_1\",\n",
    "]\n",
    "\n",
    "\n",
    "# Keep only rows where all features + target are present\n",
    "df_f = df.dropna(subset=[\"delta_f\"] + FEATURES_F)\n",
    "\n",
    "Xf = df_f[FEATURES_F]\n",
    "yf = df_f[\"delta_f\"]\n",
    "\n",
    "# ─── 4) TRAIN / EVAL ────────────────────────────────────────────────────────────\n",
    "Xf_train, Xf_val, yf_train, yf_val = train_test_split(\n",
    "    Xf, yf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_f = RandomForestRegressor(n_estimators=150, random_state=42)\n",
    "model_f.fit(Xf_train, yf_train)\n",
    "\n",
    "yf_pred = model_f.predict(Xf_val)\n",
    "mse = mean_squared_error(yf_val, yf_pred)\n",
    "\n",
    "print(f\"ΔF samples={len(Xf):,}  MSE={mse:.4f}\")\n",
    "\n",
    "# ─── 5) SAVE MODEL ─────────────────────────────────────────────────────────────\n",
    "joblib.dump(model_f, \"predictingModels/delta_f_model.pkl\")\n",
    "print(\"ΔF model trained & saved as predictingModels/delta_f_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca9e78",
   "metadata": {},
   "source": [
    "## 0.3 Train risk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Load and sort dataset\n",
    "df = pd.read_csv(\"data/soccer_data_cleaned.csv\", parse_dates=[\"date\"])\n",
    "df = df.sort_values([\"player_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Ensure injury_flag is integer for logical ops\n",
    "df[\"injury_flag\"] = df[\"injury_flag\"].fillna(0).astype(int)\n",
    "\n",
    "# Create future injury proxy (injury in next 3 days)\n",
    "grouped = df.groupby(\"player_id\")[\"injury_flag\"]\n",
    "df[\"injury_within_3\"] = (\n",
    "    grouped.shift(-1).fillna(0).astype(int) |\n",
    "    grouped.shift(-2).fillna(0).astype(int) |\n",
    "    grouped.shift(-3).fillna(0).astype(int)\n",
    ").astype(int)\n",
    "\n",
    "# Drop rows with null target\n",
    "df = df.dropna(subset=[\"injury_within_3\"])\n",
    "\n",
    "# Select features\n",
    "FEATURES = [\n",
    "    \"load\",\n",
    "    \"fatigue_post\",\n",
    "    \"sleep_duration\",\n",
    "    \"sleep_quality\",\n",
    "    \"stress\",\n",
    "    \"is_rest_day\",\n",
    "    \"injury_flag_lag_1\",\n",
    "    \"load_rolling_7\",\n",
    "    \"total_duration\",\n",
    "    \"fatigue_post_rolling_7\"\n",
    "]\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[\"injury_within_3\"]\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imp = pd.DataFrame(imputer.fit_transform(X), columns=FEATURES)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_imp, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Train classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_val)\n",
    "y_prob = clf.predict_proba(X_val)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_val, y_prob))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Save pipeline\n",
    "# train_injury_classifier.py (fix this section if needed)\n",
    "joblib.dump({\n",
    "    \"model\": clf,             \n",
    "    \"features\": list(X.columns)\n",
    "}, \"predictingModels/delta_r_classifier.pkl\")\n",
    "\n",
    "print(\"Risk classifier saved as predictingModels/delta_r_classifier.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c63e86",
   "metadata": {},
   "source": [
    "## 0.4 Train risk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_load_per_minute.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_load_per_minute(csv_path=\"data/soccer_data_cleaned.csv\"):\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "\n",
    "    # Map intensity from action values\n",
    "    action_to_intensity = {\"Train_Low\": 0.3, \"Train_Med\": 0.6, \"Train_High\": 0.9}\n",
    "    df[\"intensity\"] = df[\"action\"].map(action_to_intensity)\n",
    "\n",
    "    # Filter to only rows with known training intensities\n",
    "    train_df = df.dropna(subset=[\"intensity\", \"load\", \"total_duration\"])\n",
    "\n",
    "    # Aggregate total load and duration per intensity value\n",
    "    stats = (\n",
    "        train_df\n",
    "        .groupby(\"intensity\")\n",
    "        .agg(total_load=(\"load\", \"sum\"), total_duration=(\"total_duration\", \"sum\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create dictionary mapping intensity -> load per minute\n",
    "    load_per_minute = {\n",
    "        round(row.intensity, 2): row.total_load / row.total_duration\n",
    "        for _, row in stats.iterrows()\n",
    "    }\n",
    "\n",
    "    return load_per_minute\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_map = calculate_load_per_minute()\n",
    "    for intensity, rate in load_map.items():\n",
    "        print(f\"Intensity {intensity}: Load/min = {rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b200d68",
   "metadata": {},
   "source": [
    "## 1. Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eea2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import queue\n",
    "import time\n",
    "import heapq\n",
    "import functools\n",
    "import threading\n",
    "\n",
    "global_df = defaultdict(list)\n",
    "global_dp = defaultdict(list)\n",
    "global_prob = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7516a",
   "metadata": {},
   "source": [
    "## 2. Athlete State Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A node in the search tree representing a state in the search space.\n",
    "    \n",
    "    Each node contains a state (day, fatigue, risk, performance, history),\n",
    "    a reference to its parent node, the action that led to this state,\n",
    "    and various cost metrics used by search algorithms.\n",
    "    \n",
    "    Attributes:\n",
    "        state: Tuple containing (day, fatigue, risk, performance, history)\n",
    "        parent: Reference to the parent Node\n",
    "        action: The action (intensity, duration) that led to this state\n",
    "        g: Path cost from start to this node\n",
    "        f: Total evaluation function value (g + h)\n",
    "        h: Heuristic value (estimated cost to goal)\n",
    "        depth: Depth of this node in the search tree\n",
    "    \"\"\"\n",
    "    def __init__(self, state, parent=None, action=None, g=0, f=0, h=0, costless=False):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        if costless == False:\n",
    "            self.g = g\n",
    "            self.f = f\n",
    "            self.h = h\n",
    "\n",
    "        if parent is None:\n",
    "            self.depth = 0\n",
    "        else:\n",
    "            self.depth = parent.depth + 1\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.state)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Node) and self.state == other.state\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return isinstance(other, Node) and self.f > other.f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82f7a0",
   "metadata": {},
   "source": [
    "## 3. Athlete Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AthletePerformanceProblem:\n",
    "    \"\"\"\n",
    "    Search problem for athlete performance planning using learned ΔF, ΔP, ΔR models.\n",
    "    State: (day, fatigue, risk, performance, history)\n",
    "    Actions: Train (intensity, duration) or Rest: (0.0, 0.0)\n",
    "    Transition: ML regression/classification models via simulate_step logic.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 initial_state: tuple = (0, 1.0, 0.1, 5.0),\n",
    "                 target_day: int = 14,\n",
    "                 genetic: bool = False):\n",
    "        # Load models\n",
    "        if genetic:\n",
    "            self.delta_f = joblib.load(\"genetic_model/delta_f_model.pkl\")\n",
    "            self.delta_p = joblib.load(\"genetic_model/delta_p_model.pkl\")\n",
    "            r_loaded = joblib.load(\"predictingModels/delta_r_classifier.pkl\")\n",
    "        else:\n",
    "            self.delta_f = joblib.load(\"predictingModels/delta_f_model.pkl\")\n",
    "            self.delta_p = joblib.load(\"predictingModels/delta_p_model.pkl\")\n",
    "            r_loaded = joblib.load(\"predictingModels/delta_r_classifier.pkl\")\n",
    "\n",
    "        # Unpack classifier\n",
    "        if hasattr(r_loaded, 'predict_proba') and hasattr(r_loaded, 'feature_names_in_'):\n",
    "            self.delta_r = r_loaded\n",
    "            self.r_feats = list(r_loaded.feature_names_in_)\n",
    "        elif isinstance(r_loaded, dict):\n",
    "            for v in r_loaded.values():\n",
    "                if hasattr(v, 'predict_proba') and hasattr(v, 'feature_names_in_'):\n",
    "                    self.delta_r = v\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    self.r_feats = list(v)\n",
    "        else:\n",
    "            raise ValueError(\"Unable to extract injury classifier and features\")\n",
    "        # Compute load-per-minute mapping\n",
    "        self.LOAD_PER_MIN = calculate_load_per_minute()\n",
    "        # Defaults\n",
    "        self.SLEEP_DUR = 7.5\n",
    "        self.SLEEP_QLT = 3.0\n",
    "        self.STRESS    = 2.5\n",
    "        self.f_feats = list(self.delta_f.feature_names_in_)\n",
    "        self.p_feats = list(self.delta_p.feature_names_in_)\n",
    "        # Initialize state history\n",
    "        day, f, r, p = initial_state\n",
    "        self.initial_state = (day, f, r, p, [\n",
    "            {'load': 0.0,\n",
    "             'fatigue': f,\n",
    "             'injury_count': 0,\n",
    "             'days_since_game': 0,\n",
    "             'days_since_last_injury': 0}\n",
    "        ])\n",
    "\n",
    "        self.target_day = target_day\n",
    "\n",
    "    def actions(self):\n",
    "        train_actions = [(i, d) for i in (0.3, 0.6, 0.9) for d in (60, 120)]\n",
    "        return train_actions + [(0.0, 0.0)]  # rest\n",
    "\n",
    "    def apply_action(self, state, action):\n",
    "        # Unpack\n",
    "        day, F, R, P, history = state\n",
    "        intensity, duration = action\n",
    "        is_rest = (intensity == 0.0 and duration == 0.0)\n",
    "        # Compute load\n",
    "        load = 0.0\n",
    "        if not is_rest:\n",
    "            load = self.LOAD_PER_MIN.get(intensity, 0.0) * duration\n",
    "        # Rolling-7 calculations\n",
    "        last7 = history[-7:]\n",
    "        load7 = np.mean([h['load'] for h in last7] + [load])\n",
    "        fat7  = np.mean([h['fatigue'] for h in last7] + [F])\n",
    "        prev = history[-1]\n",
    "        inj_lag1 = int(prev['injury_count'] > 0)\n",
    "        # Assemble features\n",
    "        feat = {\n",
    "            'load': load,\n",
    "            'action_intensity': intensity,\n",
    "            'fatigue_post': F,\n",
    "            'performance_lag_1': P,\n",
    "            'sleep_duration': self.SLEEP_DUR,\n",
    "            'sleep_quality':  self.SLEEP_QLT,\n",
    "            'stress':         self.STRESS,\n",
    "            'is_rest_day':    int(is_rest),\n",
    "            'injury_flag_lag_1': inj_lag1,\n",
    "            'load_rolling_7':      load7,\n",
    "            'fatigue_post_rolling_7': fat7,\n",
    "            'sleep_duration_rolling_7': self.SLEEP_DUR,\n",
    "            'sleep_quality_rolling_7':  self.SLEEP_QLT,\n",
    "            'stress_rolling_7':        self.STRESS,\n",
    "            'load_lag_1':      prev['load'],\n",
    "            'total_duration':  duration,\n",
    "            'injury_count':    prev['injury_count'],\n",
    "            'days_since_game': prev['days_since_game'] + 1,\n",
    "            'days_since_last_injury': prev['days_since_last_injury'] + 1\n",
    "        }\n",
    "        X = pd.DataFrame([feat])\n",
    "        # Predictions\n",
    "        \n",
    "        if is_rest:\n",
    "            Rn = np.clip(R * 0.86, 0.0, 1.0)\n",
    "            Fn = max(F * 0.85, 0.0)\n",
    "            Pn = max(P * 0.91, 0.0)\n",
    "        else:\n",
    "            dF = float(self.delta_f.predict(X[self.f_feats])[0])\n",
    "            dP = float(self.delta_p.predict(X[self.p_feats])[0])\n",
    "            prob = self.delta_r.predict_proba(X[self.r_feats])[0, 1]\n",
    "            global_df[tuple(action)].append(dF)\n",
    "            global_dp[tuple(action)].append(dP)\n",
    "            global_prob[tuple(action)].append(prob)\n",
    "            Rn = np.clip(R + prob, 0.0, 1.0)\n",
    "            Fn = np.clip(F + dF, 0.0, 5.0)\n",
    "            Pn = np.clip(P + dP, 0.0, 10.0)\n",
    "\n",
    "        # Update history\n",
    "        new_rec = {\n",
    "            'load': load,\n",
    "            'fatigue': Fn,\n",
    "            'injury_count': prev['injury_count'],\n",
    "            'days_since_game': feat['days_since_game'],\n",
    "            'days_since_last_injury': feat['days_since_last_injury']\n",
    "        }\n",
    "        new_history = history + [new_rec]\n",
    "        return (day + 1, Fn, Rn, Pn, new_history)\n",
    "    \n",
    "    def expand_node(self, node,use_cost=False, use_heuristic=False):\n",
    "        \"\"\"\n",
    "        Expands a node by applying all possible actions and returning the resulting nodes\n",
    "        \"\"\"\n",
    "        children = []\n",
    "        for action in self.actions():\n",
    "            new_state = self.apply_action(node.state, action)\n",
    "            if self.is_valid(new_state):\n",
    "                cost = self.cost(node.state, action) if use_cost else 0\n",
    "                heuristic = self.heuristic(new_state) if use_heuristic else 0\n",
    "                child_node = Node(new_state, parent=node, action=action, cost=cost, f=cost + heuristic)\n",
    "                children.append(child_node)\n",
    "        return children\n",
    "\n",
    "    def is_valid(self, state):\n",
    "        day, fatigue, risk, performance, _ = state\n",
    "        return (\n",
    "                0 <= fatigue <= 5.0\n",
    "                and 0 <= risk <= 1.0\n",
    "                and 0 <= performance <= 10.0)\n",
    "\n",
    "    def is_goal(self, state):\n",
    "        day, fatigue, risk, performance, _ = state\n",
    "        return (day == self.target_day\n",
    "                and performance >= self.target_perf\n",
    "                and fatigue <= self.max_fatigue\n",
    "                and risk <= self.max_risk)\n",
    "    \n",
    "    def cost(self, state, action):\n",
    "        \"\"\"\n",
    "        Calculate the cost of applying an action to the current state.\n",
    "        \n",
    "        This function evaluates the immediate cost of applying an action, balancing:\n",
    "        - Performance improvement (negative cost/benefit)\n",
    "        - Increase in fatigue (cost)\n",
    "        - Increase in injury risk (cost)\n",
    "        - Training load (cost proportional to intensity×duration)\n",
    "        \n",
    "        Lower cost values indicate better actions.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (day, fatigue, risk, performance, history)\n",
    "            action: Action to apply (intensity, duration)\n",
    "            \n",
    "        Returns:\n",
    "            Numerical cost value (lower is better)\n",
    "        \"\"\"\n",
    "        day, fatigue, risk, performance, _ = state\n",
    "        intensity, duration = action\n",
    "        is_rest = (intensity == 0.0 and duration == 0.0)\n",
    "        \n",
    "        # Apply action to get new state\n",
    "        new_state = self.apply_action(state, action)\n",
    "        _, new_fatigue, new_risk, new_perf, _ = new_state\n",
    "        \n",
    "        # Calculate deltas (changes in state)\n",
    "        delta_fatigue = new_fatigue - fatigue\n",
    "        delta_risk = new_risk - risk\n",
    "        delta_perf = new_perf - performance\n",
    "        \n",
    "        if is_rest:\n",
    "            # For rest days, prioritize recovery (fatigue reduction)\n",
    "            recovery_efficiency = max(0, fatigue - new_fatigue)\n",
    "            cost = 2.0 - (3.0 * recovery_efficiency)\n",
    "            # Small penalty if performance drops significantly during rest\n",
    "            if delta_perf < -1.0:\n",
    "                cost += 1.0\n",
    "        else:\n",
    "            # For training days, calculate efficiency metrics\n",
    "            # Higher cost if no performance improvement\n",
    "            if delta_perf <= 0:\n",
    "                perf_factor = 5.0\n",
    "            else:\n",
    "                # Performance efficiency: lower cost for more performance gain relative to fatigue/risk\n",
    "                # Add small constant to avoid division by zero\n",
    "                fatigue_risk_sum = max(0.01, delta_fatigue + (delta_risk * 4.0))\n",
    "                perf_factor = 2.0 - min(2.0, delta_perf / fatigue_risk_sum)\n",
    "            \n",
    "            # Risk penalty increases exponentially as we approach maximum risk\n",
    "            risk_proximity = new_risk\n",
    "            risk_penalty = 2.0 * (risk_proximity ** 2)\n",
    "            \n",
    "            # Fatigue penalty increases as we approach maximum fatigue (assumed to be 5.0)\n",
    "            fatigue_proximity = new_fatigue / 5.0\n",
    "            fatigue_penalty = 1.5 * (fatigue_proximity ** 2)\n",
    "            \n",
    "            # Combined cost (lower is better)\n",
    "            cost = perf_factor + risk_penalty + fatigue_penalty\n",
    "            \n",
    "            # Add penalty for excessive training load\n",
    "            training_load = intensity * duration\n",
    "            if training_load > 80:\n",
    "                cost += 0.5 * (training_load - 80) / 20\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def heuristic(self, state) -> float:\n",
    "        \"\"\"\n",
    "        Estimate how close the current state is to the goal state.\n",
    "        \n",
    "        This function provides a heuristic that considers:\n",
    "        1. Performance deficit from target\n",
    "        2. Days remaining to reach target\n",
    "        3. Current fatigue and risk levels\n",
    "        4. Potential for improvement over remaining days\n",
    "        \n",
    "        Lower heuristic values indicate more promising states.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (day, fatigue, risk, performance, history)\n",
    "            \n",
    "        Returns:\n",
    "            Numerical heuristic value (lower is better)\n",
    "        \"\"\"\n",
    "        day, fatigue, risk, performance, _ = state\n",
    "        \n",
    "        # Calculate remaining days until target\n",
    "        remaining_days = max(0, self.target_day - day)\n",
    "        \n",
    "        # Calculate performance deficit from target\n",
    "        perf_deficit = max(0, self.target_perf - performance)\n",
    "                \n",
    "        # If already at the target day, evaluate based on goal conditions\n",
    "        if remaining_days == 0:\n",
    "            # If performance goal met and constraints satisfied, heuristic is 0\n",
    "            if (performance >= self.target_perf and \n",
    "                fatigue <= self.max_fatigue and \n",
    "                risk <= self.max_risk):\n",
    "                return 0.0\n",
    "            \n",
    "            # Otherwise, return a value based on how far we are from satisfying all conditions\n",
    "            return (\n",
    "                3.0 * perf_deficit + \n",
    "                2.0 * max(0, fatigue - self.max_fatigue) +\n",
    "                2.0 * max(0, risk - self.max_risk)\n",
    "            )\n",
    "        \n",
    "        # For states before the target day, estimate based on trajectory\n",
    "        \n",
    "        # Estimate max potential performance improvement per day, simplified and should be studied from the transition model\n",
    "        max_improvement_per_day = 0.3\n",
    "        \n",
    "        # Estimate if we can reach the performance target in time\n",
    "        potential_improvement = max_improvement_per_day * remaining_days\n",
    "        if potential_improvement < perf_deficit:\n",
    "            # Cannot reach target with max improvement rate, so increase heuristic\n",
    "            reachability_penalty = 2.0 * (perf_deficit - potential_improvement)\n",
    "        else:\n",
    "            reachability_penalty = 0.0\n",
    "        \n",
    "        # Risk and fatigue penalties increase as we get closer to max allowed values\n",
    "        risk_proximity = risk / self.max_risk if hasattr(self, 'max_risk') else risk\n",
    "        fatigue_proximity = fatigue / self.max_fatigue if hasattr(self, 'max_fatigue') else fatigue / 5.0\n",
    "        \n",
    "        risk_penalty = 1.5 * risk_proximity**2\n",
    "        fatigue_penalty = 1.0 * fatigue_proximity**2\n",
    "        \n",
    "        # Days factor - prioritize states that have made more progress toward goal\n",
    "        days_factor = 0.8 * (1.0 - day / self.target_day)\n",
    "        \n",
    "        # Combined heuristic - lower values are better\n",
    "        return perf_deficit + reachability_penalty + risk_penalty + fatigue_penalty + days_factor\n",
    "\n",
    "    def random_individual(self): #Genetic\n",
    "        \"\"\"\n",
    "        Create a random training schedule for the target number of days.\n",
    "        \n",
    "        Returns:\n",
    "            A tuple of (intensity, duration) pairs representing a training schedule\n",
    "        \"\"\"\n",
    "        # Default to 14 days if target_day is not set\n",
    "        days = getattr(self, 'target_day', 14)\n",
    "        \n",
    "        # Possible intensities and durations\n",
    "        intensities = [0.0, 0.3, 0.6, 0.9]  # Including rest days (0.0)\n",
    "        durations = [0, 30, 60, 90, 120]    # 0 for rest days\n",
    "        \n",
    "        # Generate random schedule\n",
    "        schedule = []\n",
    "        for _ in range(days):\n",
    "            intensity = random.choice(intensities)\n",
    "            # If it's a rest day, duration is 0\n",
    "            duration = 0 if intensity == 0.0 else random.choice(durations[1:])\n",
    "            schedule.append((intensity, duration))\n",
    "        return list(schedule)\n",
    "\n",
    "    def evaluate_individual(self, indiv): #Genetic\n",
    "        \n",
    "        current_state = self.initial_state\n",
    "        individual = indiv[:]\n",
    "        while individual:\n",
    "            indiv_action = individual.pop(0)\n",
    "            current_state = self.apply_action(current_state, indiv_action) \n",
    "\n",
    "        return current_state[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa22b16",
   "metadata": {},
   "source": [
    "## 4. Search Algorithms\n",
    "\n",
    "Explore and implement various search algorithms to optimize the training schedule for an athlete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d701a54",
   "metadata": {},
   "source": [
    "### 4.1. Uninformed Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708422cd",
   "metadata": {},
   "source": [
    "#### Best First Search (BFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFSSearch:\n",
    "    \"\"\"\n",
    "    Implementation of Breadth-First Search algorithm for athlete training plans.\n",
    "    \n",
    "    This class performs a breadth-first search to find an optimal training plan\n",
    "    by exploring all nodes at the current depth before moving to nodes at the next depth.\n",
    "    This ensures the shortest path (in terms of number of actions) is found.\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        \"\"\"\n",
    "        Initialize the BFS search algorithm with a problem instance.\n",
    "        \n",
    "        Args:\n",
    "            problem: An AthletePerformanceProblem instance\n",
    "        \"\"\"\n",
    "        self.problem = problem\n",
    "        self.expanded_nodes = 0\n",
    "        self.max_queue_size = 0\n",
    "        \n",
    "        # Set target day and performance (needed for is_goal)\n",
    "        self.problem.target_day = 10\n",
    "        self.problem.target_perf = 6.5\n",
    "        self.problem.max_fatigue = 2.7\n",
    "        self.problem.max_risk = 0.3\n",
    "        \n",
    "    def search(self, max_depth=float('inf')):\n",
    "        \"\"\"\n",
    "        Perform breadth-first search to find an optimal training plan.\n",
    "        \"\"\"\n",
    "        start_node = Node(state=self.problem.initial_state, costless=True)\n",
    "        \n",
    "        # Use deque for more efficient BFS queue\n",
    "        frontier = deque([start_node])\n",
    "        \n",
    "        # Track explored states to avoid cycles\n",
    "        explored = set()\n",
    "        \n",
    "        while frontier:\n",
    "            # Get next node to explore (FIFO for BFS)\n",
    "            current_node = frontier.popleft()\n",
    "            \n",
    "            day, _, _, performance, _ = current_node.state\n",
    "            # Check if goal state (using custom goal check)\n",
    "            if day >= self.problem.target_day and performance >= self.problem.target_perf:\n",
    "                return current_node\n",
    "                \n",
    "            # Skip already explored states\n",
    "            rounded_state = self._round_state(current_node.state)\n",
    "            if rounded_state in explored:\n",
    "                continue\n",
    "            \n",
    "            # Mark this state as explored\n",
    "            explored.add(rounded_state)\n",
    "            \n",
    "            # Get the valid actions from the current state\n",
    "            for action in self.problem.actions():\n",
    "                # Apply the action to get a new state\n",
    "                # Create a state that includes history, as expected by the updated apply_action method\n",
    "                current_state = current_node.state  # Full state with history\n",
    "                new_state = self.problem.apply_action(current_state, action)\n",
    "                \n",
    "                # Skip invalid states\n",
    "                if not self.is_valid(new_state):\n",
    "                    continue\n",
    "                \n",
    "                # Create a new node for this state\n",
    "                child_node = Node(new_state, parent=current_node, action=action, costless=True)\n",
    "                \n",
    "                # Skip if exceeds maximum depth\n",
    "                if child_node.depth > max_depth:\n",
    "                    continue\n",
    "                    \n",
    "                # Add to frontier for further exploration\n",
    "                frontier.append(child_node)\n",
    "            \n",
    "            self.expanded_nodes += 1\n",
    "            \n",
    "            # Track maximum queue size\n",
    "            self.max_queue_size = max(self.max_queue_size, len(frontier))\n",
    "            \n",
    "            # Progress indicator\n",
    "            if self.expanded_nodes % 500 == 0:\n",
    "                print(f\"Explored {self.expanded_nodes} nodes, queue size: {len(frontier)}\")\n",
    "            \n",
    "        # If we've examined all nodes and haven't found a solution, return None\n",
    "        return None\n",
    "    \n",
    "    def is_valid(self, state):\n",
    "        \"\"\"Check if a state is valid based on constraints.\"\"\"\n",
    "        _, fatigue, risk, _, _ = state\n",
    "        return fatigue <= self.problem.max_fatigue and risk <= self.problem.max_risk\n",
    "        \n",
    "    def _round_state(self, state):\n",
    "        \"\"\"\n",
    "        Round state values to reduce the state space and avoid similar states.\n",
    "        \"\"\"\n",
    "        day, fatigue, risk, performance, _ = state\n",
    "        return (\n",
    "            day,\n",
    "            round(fatigue, 1),  # Round fatigue to 1 decimal place\n",
    "            round(risk, 1),     # Round risk to 1 decimal place\n",
    "            round(performance, 1)  # Round performance to 1 decimal place\n",
    "        )\n",
    "\n",
    "    def reconstruct_path(self, node):\n",
    "        \"\"\"\n",
    "        Reconstruct the path from the initial state to the goal state.\n",
    "        \"\"\"\n",
    "        path = []\n",
    "        while node and node.parent:\n",
    "            path.append(node.action)\n",
    "            node = node.parent\n",
    "        return path[::-1]  # reverse to get the correct order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b3e21",
   "metadata": {},
   "source": [
    "#### Depth First Search (DFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd267703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFSSearch:\n",
    "    \"\"\"\n",
    "    Implementation of Depth-First Search algorithm for athlete training plans.\n",
    "    \n",
    "    This class performs a depth-first search to find a training plan\n",
    "    by exploring as far as possible along each branch before backtracking.\n",
    "    DFS can find solutions quickly in some cases but does not guarantee optimality.\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        \"\"\"\n",
    "        Initialize the DFS search algorithm with a problem instance.\n",
    "        \n",
    "        Args:\n",
    "            problem: An AthletePerformanceProblem instance\n",
    "        \"\"\"\n",
    "        self.problem = problem\n",
    "        self.expanded_nodes = 0\n",
    "        self.max_stack_size = 0\n",
    "\n",
    "        # Set target day and performance (needed for is_goal)\n",
    "        self.problem.target_day = 10\n",
    "        self.problem.target_perf = 7\n",
    "        self.problem.max_fatigue = 4\n",
    "        self.problem.max_risk = 0.3\n",
    "\n",
    "    def search(self, max_depth=10):\n",
    "        \"\"\"\n",
    "        Perform depth-first search to find a training plan.\n",
    "        \n",
    "        Args:\n",
    "            max_depth: Maximum depth to explore in the search tree\n",
    "        \n",
    "        Returns:\n",
    "            The goal node if a solution is found, None otherwise\n",
    "        \"\"\"\n",
    "        start_node = Node(state=self.problem.initial_state, costless=True)\n",
    "        # Use stack (LIFO) for DFS\n",
    "        stack = deque([start_node])\n",
    "        # Track explored states to avoid cycles\n",
    "        explored = set()\n",
    "\n",
    "        while stack:\n",
    "            # Get next node to explore (LIFO for DFS)\n",
    "            current_node = stack.pop()\n",
    "\n",
    "            day, fatigue, risk, performance, _ = current_node.state\n",
    "            # Check if goal state (using custom goal check)\n",
    "            if day >= self.problem.target_day and performance >= self.problem.target_perf:\n",
    "                print(f\"Goal found! Day: {day}, Performance: {performance:.2f}, Fatigue: {fatigue:.2f}, Risk: {risk:.2f}\")\n",
    "                return current_node\n",
    "\n",
    "            # Skip already explored states\n",
    "            rounded_state = self._round_state(current_node.state)\n",
    "            if rounded_state in explored:\n",
    "                continue\n",
    "\n",
    "            # Mark this state as explored\n",
    "            explored.add(rounded_state)\n",
    "\n",
    "            # Get the valid actions from the current state\n",
    "            for action in self.problem.actions():\n",
    "                # Apply the action to get a new state\n",
    "                current_state = current_node.state\n",
    "                new_state = self.problem.apply_action(current_state, action)\n",
    "\n",
    "                # Skip invalid states\n",
    "                if not self.is_valid(new_state):\n",
    "                    continue\n",
    "\n",
    "                # Create a new node for this state\n",
    "                child_node = Node(new_state, parent=current_node, action=action, costless=True)\n",
    "\n",
    "                # Skip if exceeds maximum depth\n",
    "                if child_node.depth > max_depth:\n",
    "                    continue\n",
    "\n",
    "                # Add to stack for further exploration\n",
    "                stack.append(child_node)\n",
    "\n",
    "            self.expanded_nodes += 1\n",
    "            # Track maximum stack size\n",
    "            self.max_stack_size = max(self.max_stack_size, len(stack))\n",
    "            \n",
    "            # Progress indicator\n",
    "            if self.expanded_nodes % 100 == 0:\n",
    "                print(f\"Explored {self.expanded_nodes} nodes, stack size: {len(stack)}, \" \n",
    "                      f\"Current state: Day {day}, F={fatigue:.2f}, R={risk:.2f}, P={performance:.2f}, Depth={current_node.depth}\")\n",
    "\n",
    "        # If we've examined all nodes and haven't found a solution, return None\n",
    "        return None\n",
    "\n",
    "    def is_valid(self, state):\n",
    "        \"\"\"\n",
    "        Check if a state is valid based on constraints.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the state is valid, False otherwise\n",
    "        \"\"\"\n",
    "        _, fatigue, risk, _, _ = state\n",
    "        return fatigue <= self.problem.max_fatigue and risk <= self.problem.max_risk\n",
    "\n",
    "    def _round_state(self, state):\n",
    "        \"\"\"\n",
    "        Round state values to reduce the state space and avoid similar states.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to round\n",
    "            \n",
    "        Returns:\n",
    "            A tuple with rounded state values\n",
    "        \"\"\"\n",
    "        day, fatigue, risk, performance, _ = state\n",
    "        return (\n",
    "            day,\n",
    "            round(fatigue, 1),  # Round fatigue to 1 decimal place\n",
    "            round(risk, 1),     # Round risk to 1 decimal place\n",
    "            round(performance, 1)  # Round performance to 1 decimal place\n",
    "        )\n",
    "\n",
    "    def reconstruct_path(self, node):\n",
    "        \"\"\"\n",
    "        Reconstruct the path from the initial state to the goal state.\n",
    "        \n",
    "        Args:\n",
    "            node: The goal node\n",
    "            \n",
    "        Returns:\n",
    "            A list of actions that lead from the initial state to the goal state\n",
    "        \"\"\"\n",
    "        path = []\n",
    "        while node and node.parent:\n",
    "            path.append(node.action)\n",
    "            node = node.parent\n",
    "        return path[::-1]  # reverse to get the correct order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60728107",
   "metadata": {},
   "source": [
    "#### Uniform Cost Search (UCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e10e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCSSearch:\n",
    "    \"\"\"\n",
    "    Implementation of Uniform Cost Search algorithm for athlete training plans.\n",
    "    \n",
    "    This class performs a uniform-cost search to find an optimal training plan\n",
    "    by exploring nodes with the lowest path cost (g-value) first. This guarantees\n",
    "    finding the lowest-cost path to the goal state, if one exists.\n",
    "    \n",
    "    Attributes:\n",
    "        problem: The AthletePerformanceProblem instance\n",
    "        expanded_nodes: Counter of nodes expanded during search\n",
    "        max_queue_size: Maximum size of the frontier queue during search\n",
    "        execution_time: Time taken to execute the search\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        \"\"\"\n",
    "        Initialize the UCS search algorithm with a problem instance.\n",
    "        \n",
    "        Args:\n",
    "            problem: An AthletePerformanceProblem instance\n",
    "        \"\"\"\n",
    "        self.problem = problem\n",
    "        self.expanded_nodes = 0\n",
    "        self.max_queue_size = 0\n",
    "        self.execution_time = 0\n",
    "        \n",
    "        # Set target day and performance\n",
    "        #self.problem.target_day = 10\n",
    "        #self.problem.target_perf = 7\n",
    "        #self.problem.max_fatigue = 3.5\n",
    "        #self.problem.max_risk = 2.5\n",
    "        \n",
    "    def search(self, max_depth=10):\n",
    "        \"\"\"\n",
    "        Performs a uniform-cost search to find an optimal training plan.\n",
    "        \n",
    "        This method expands nodes with the lowest cumulative path cost (g-value) first,\n",
    "        guaranteeing the optimal solution if one exists. It uses the problem's cost\n",
    "        function to calculate the cost of each action.\n",
    "        \n",
    "        Args:\n",
    "            max_depth: Maximum depth to explore in the search tree\n",
    "            \n",
    "        Returns:\n",
    "            The goal node if a solution is found, None otherwise\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create initial node from the problem's initial state\n",
    "        initial_node = Node(self.problem.initial_state, g=0)\n",
    "        \n",
    "        # Priority queue for UCS - priority is cumulative path cost (g-value)\n",
    "        frontier = queue.PriorityQueue()\n",
    "        frontier.put((0, initial_node))  # Initial cost is 0\n",
    "        \n",
    "        # Track explored states and their costs\n",
    "        explored = {}  # Maps rounded state to lowest g-value found\n",
    "        \n",
    "        # Best solution found so far\n",
    "        best_solution = None\n",
    "        best_cost = float('inf')\n",
    "        \n",
    "        while not frontier.empty():\n",
    "            # Get node with lowest path cost\n",
    "            current_cost, current_node = frontier.get()\n",
    "            \n",
    "            day, fatigue, risk, performance, _ = current_node.state\n",
    "            \n",
    "            # Check if we've reached the target day\n",
    "            if day == self.problem.target_day and performance >= self.problem.target_perf:\n",
    "                if current_cost < best_cost:\n",
    "                    best_solution = current_node\n",
    "                    best_cost = current_cost\n",
    "                    \n",
    "                    # Since UCS guarantees the optimal path to any node,\n",
    "                    # if we want the first solution at target day, we can return it\n",
    "                    self.execution_time = time.time() - start_time\n",
    "                    return best_solution\n",
    "            \n",
    "            # Don't explore beyond target day\n",
    "            if day > self.problem.target_day:\n",
    "                continue\n",
    "                \n",
    "            # Check if we've already found a better path to this state\n",
    "            rounded_state = self._round_state(current_node.state)\n",
    "            if rounded_state in explored and explored[rounded_state] <= current_cost:\n",
    "                continue\n",
    "            \n",
    "            # Remember this as the best path to this state so far\n",
    "            explored[rounded_state] = current_cost\n",
    "            \n",
    "            # Skip if exceeds maximum depth\n",
    "            if current_node.depth >= max_depth:\n",
    "                continue\n",
    "                \n",
    "            # Get valid actions for current state\n",
    "            for action in self.problem.actions():\n",
    "                # Apply action to get new state\n",
    "                new_state = self.problem.apply_action(current_node.state, action)\n",
    "                \n",
    "                # Skip invalid states\n",
    "                if not self.is_valid(new_state):\n",
    "                    continue\n",
    "                \n",
    "                # Calculate action cost\n",
    "                action_cost = self.problem.cost(current_node.state, action)\n",
    "                \n",
    "                # Calculate cumulative path cost\n",
    "                new_cost = current_node.g + action_cost\n",
    "                \n",
    "                # Create new node with updated cost\n",
    "                child_node = Node(\n",
    "                    state=new_state, \n",
    "                    parent=current_node, \n",
    "                    action=action, \n",
    "                    g=new_cost, \n",
    "                    f=new_cost  # In UCS, f = g (no heuristic)\n",
    "                )\n",
    "                \n",
    "                # Add to frontier with priority based on path cost\n",
    "                frontier.put((new_cost, child_node))\n",
    "            \n",
    "            self.expanded_nodes += 1\n",
    "            \n",
    "            # Track maximum queue size\n",
    "            self.max_queue_size = max(self.max_queue_size, frontier.qsize())\n",
    "            \n",
    "            # Progress indicator\n",
    "            if self.expanded_nodes % 200 == 0:\n",
    "                print(f\"Explored {self.expanded_nodes} nodes, queue size: {frontier.qsize()}\")\n",
    "        \n",
    "        self.execution_time = time.time() - start_time\n",
    "        return best_solution\n",
    "    \n",
    "    def is_valid(self, state):\n",
    "        \"\"\"\n",
    "        Check if a state is valid based on constraints.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to validate\n",
    "            \n",
    "        Returns:\n",
    "            True if state satisfies all constraints, False otherwise\n",
    "        \"\"\"\n",
    "        _, fatigue, risk, _, _ = state\n",
    "        return fatigue <= self.problem.max_fatigue and risk <= self.problem.max_risk\n",
    "        \n",
    "    def _round_state(self, state):\n",
    "        \"\"\"\n",
    "        Round state values to reduce the state space and avoid similar states.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to round\n",
    "            \n",
    "        Returns:\n",
    "            A tuple with rounded values (excluding history)\n",
    "        \"\"\"\n",
    "        day, fatigue, risk, performance, _ = state\n",
    "        return (\n",
    "            day,\n",
    "            round(fatigue, 1),  # Round fatigue to 1 decimal place\n",
    "            round(risk, 1),     # Round risk to 1 decimal place\n",
    "            round(performance, 1)  # Round performance to 1 decimal place\n",
    "        )\n",
    "    \n",
    "    def reconstruct_path(self, node):\n",
    "        \"\"\"\n",
    "        Reconstruct the path from the initial state to the goal state.\n",
    "        \n",
    "        Args:\n",
    "            node: The goal node\n",
    "            \n",
    "        Returns:\n",
    "            A list of actions from start to goal in correct order\n",
    "        \"\"\"\n",
    "        path = []\n",
    "        while node and node.parent:\n",
    "            path.append(node.action)\n",
    "            node = node.parent\n",
    "        return path[::-1]  # Reverse to get the correct order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3916fc1",
   "metadata": {},
   "source": [
    "### 4.2. Informed Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f4254",
   "metadata": {},
   "source": [
    "#### Greedy Best First Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearch:\n",
    "    \"\"\"\n",
    "    Implementation of a greedy best-first search algorithm for athlete training plans.\n",
    "    \n",
    "    This class performs a greedy best-first search to find an optimal training plan\n",
    "    by using a heuristic to guide the search toward promising states. The search\n",
    "    expands nodes based on their heuristic values, always exploring the most\n",
    "    promising node first.\n",
    "    \n",
    "    Attributes:\n",
    "        problem: The AthletePerformanceProblem instance\n",
    "        expanded_nodes: Counter of nodes expanded during search\n",
    "        max_queue_size: Maximum size of the frontier queue during search\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.expanded_nodes = 0\n",
    "        self.max_queue_size = 0\n",
    "        self.execution_time = 0\n",
    "        \n",
    "        #self.problem.target_day = 14\n",
    "        #self.problem.target_perf = 9\n",
    "        #self.problem.max_fatigue = 3.5\n",
    "        #self.problem.max_risk = 0.5\n",
    "        \n",
    "    def search(self, max_depth=float('inf')):\n",
    "        \"\"\"\n",
    "        Performs a greedy best-first search to find an optimal training plan.\n",
    "        \n",
    "        This method uses generates successor nodes\n",
    "        and explores them based on their heuristic values (greedy best-first search).\n",
    "        \n",
    "        Args:\n",
    "            max_depth: Maximum depth to explore in the search tree\n",
    "            \n",
    "        Returns:\n",
    "            The goal node if a solution is found, None otherwise\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create initial node from the problem's initial state\n",
    "        initial_node = Node(self.problem.initial_state)\n",
    "        \n",
    "        # Priority queue for greedy best-first search\n",
    "        frontier = queue.PriorityQueue()\n",
    "        frontier.put((self._get_priority(initial_node), initial_node))\n",
    "        \n",
    "        # Track explored states to avoid cycles\n",
    "        explored = set()\n",
    "        \n",
    "        while not frontier.empty():\n",
    "            # Get next node to explore (with lowest priority/heuristic value)\n",
    "            _, current_node = frontier.get()\n",
    "            \n",
    "            day, _, _, performance, _ = current_node.state\n",
    "            \n",
    "            # Check if goal state\n",
    "            if day == self.problem.target_day and performance >= self.problem.target_perf:\n",
    "                # Exact day match with performance target - ideal solution\n",
    "                self.execution_time = time.time() - start_time\n",
    "                return current_node\n",
    "                            \n",
    "            # Skip already explored states\n",
    "            rounded_state = self._round_state(current_node.state)\n",
    "            if rounded_state in explored:\n",
    "                continue\n",
    "            \n",
    "            # Mark this state as explored\n",
    "            explored.add(rounded_state)\n",
    "            \n",
    "            # Skip if exceeds maximum depth or target day\n",
    "            if current_node.depth >= max_depth or day >= self.problem.target_day:\n",
    "                continue\n",
    "                \n",
    "            # Get the valid actions from the current state\n",
    "            for action in self.problem.actions():\n",
    "                # Apply action to get a new state\n",
    "                # Pass the complete state to apply_action\n",
    "                new_state = self.problem.apply_action(current_node.state, action)\n",
    "                \n",
    "                # Skip invalid states\n",
    "                if not self.is_valid(new_state):\n",
    "                    continue\n",
    "                \n",
    "                # Create a new node for this state\n",
    "                child_node = Node(new_state, parent=current_node, action=action)\n",
    "                \n",
    "                # Add to frontier with priority based on heuristic\n",
    "                frontier.put((self._get_priority(child_node), child_node))\n",
    "            \n",
    "            self.expanded_nodes += 1\n",
    "            \n",
    "            # Track maximum queue size\n",
    "            self.max_queue_size = max(self.max_queue_size, frontier.qsize())\n",
    "            \n",
    "            # Progress indicator\n",
    "            if self.expanded_nodes % 500 == 0:\n",
    "                print(f\"Explored {self.expanded_nodes} nodes, queue size: {frontier.qsize()}\")\n",
    "        \n",
    "        self.execution_time = time.time() - start_time\n",
    "                # If we've examined all nodes and haven't found a solution, return None\n",
    "        return None\n",
    "    \n",
    "    def _get_priority(self, node):\n",
    "        \"\"\"\n",
    "        Calculate priority for a node based on problem heuristic.\n",
    "        Lower values have higher priority in the queue.\n",
    "        \"\"\"\n",
    "        return self.problem.heuristic(node.state)\n",
    "    \n",
    "    def is_valid(self, state):\n",
    "        \"\"\"Check if a state is valid based on constraints.\"\"\"\n",
    "        _, fatigue, risk, _, _ = state\n",
    "        return fatigue <= self.problem.max_fatigue and risk <= self.problem.max_risk\n",
    "        \n",
    "    def _round_state(self, state):\n",
    "        \"\"\"\n",
    "        Round state values to reduce the state space and avoid similar states.\n",
    "        \"\"\"\n",
    "        day, fatigue, risk, performance, _ = state\n",
    "        return (\n",
    "            day,\n",
    "            round(fatigue, 1),  # Round fatigue to 1 decimal place\n",
    "            round(risk, 1),     # Round risk to 1 decimal place\n",
    "            round(performance, 1)  # Round performance to 1 decimal place\n",
    "        )\n",
    "    \n",
    "    def reconstruct_path(self, node):\n",
    "        \"\"\"\n",
    "        Reconstruct the path from the initial state to the goal state.\n",
    "        \n",
    "        This function traces back from the goal node to the start node\n",
    "        using parent references, collecting actions along the way.\n",
    "        \n",
    "        Args:\n",
    "            node: The goal node\n",
    "            \n",
    "        Returns:\n",
    "            A list of actions from start to goal in correct order\n",
    "        \"\"\"\n",
    "        path = []\n",
    "        while node and node.parent:\n",
    "            path.append(node.action)\n",
    "            node = node.parent\n",
    "        return path[::-1]  # reverse to get the correct order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3871d2",
   "metadata": {},
   "source": [
    "#### A Star Search (A*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a95c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AStarSearch:\n",
    "    \"\"\"\n",
    "    A* search algorithm implementation with ML prediction caching for efficiency.\n",
    "    Uses a priority queue to explore nodes in order of f-score (g + h).\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.heuristic_cache = {}  # Cache for heuristic values\n",
    "        self.expanded_nodes = 0\n",
    "        self.max_queue_size = 0\n",
    "        self.execution_time = 0\n",
    "        \n",
    "        # ML model prediction cache\n",
    "        self.prediction_cache = {}\n",
    "        self.prediction_lock = threading.Lock()  # Lock for thread safety\n",
    "        self.prediction_hits = 0\n",
    "        self.prediction_misses = 0\n",
    "        \n",
    "        # Set default target parameters if not already set\n",
    "        if not hasattr(self.problem, 'target_day'):\n",
    "            self.problem.target_day = 14\n",
    "        if not hasattr(self.problem, 'target_perf'):\n",
    "            self.problem.target_perf = 7.0\n",
    "        if not hasattr(self.problem, 'max_fatigue'):\n",
    "            self.problem.max_fatigue = 3.5\n",
    "        if not hasattr(self.problem, 'max_risk'):\n",
    "            self.problem.max_risk = 0.5\n",
    "        \n",
    "        # Apply monkey patch to problem's apply_action for caching\n",
    "        self._patch_apply_action()\n",
    "    \n",
    "    def _patch_apply_action(self):\n",
    "        \"\"\"Monkey patch the problem's apply_action method to add caching\"\"\"\n",
    "        original_apply_action = self.problem.apply_action\n",
    "        \n",
    "        @functools.wraps(original_apply_action)\n",
    "        def cached_apply_action(state, action):\n",
    "            # Create a cache key from relevant state components and action\n",
    "            day, fatigue, risk, performance, history = state\n",
    "            \n",
    "            # Use only last state for caching to reduce key size\n",
    "            if history and len(history) > 0:\n",
    "                last_history_item = history[-1]\n",
    "            else:\n",
    "                last_history_item = {}\n",
    "                \n",
    "            # Create a hashable cache key\n",
    "            cache_key = (day, round(fatigue, 2), round(risk, 2), round(performance, 2), \n",
    "                        tuple(sorted(last_history_item.items())) if last_history_item else None,\n",
    "                        action)\n",
    "            \n",
    "            # Check if we already computed this state transition\n",
    "            with self.prediction_lock:\n",
    "                if cache_key in self.prediction_cache:\n",
    "                    self.prediction_hits += 1\n",
    "                    return self.prediction_cache[cache_key]\n",
    "                \n",
    "                # If not in cache, compute it\n",
    "                self.prediction_misses += 1\n",
    "                result = original_apply_action(state, action)\n",
    "                \n",
    "                # Cache the result if cache size is reasonable\n",
    "                if len(self.prediction_cache) < 100000:  # Set a reasonable cache size limit\n",
    "                    self.prediction_cache[cache_key] = result\n",
    "                    \n",
    "                return result\n",
    "                \n",
    "        # Replace the original method with our cached version\n",
    "        self.problem.apply_action = cached_apply_action\n",
    "    \n",
    "    def get_state_key(self, state):\n",
    "        \"\"\"Creates a hashable key from a state by rounding numeric values.\"\"\"\n",
    "        day, fatigue, risk, performance, _ = state\n",
    "        return (day, round(fatigue, 1), round(risk, 2), round(performance, 1))\n",
    "    \n",
    "    def get_heuristic(self, state):\n",
    "        \"\"\"Gets the heuristic value for a state, using cache when possible.\"\"\"\n",
    "        state_key = self.get_state_key(state)\n",
    "        if state_key in self.heuristic_cache:\n",
    "            return self.heuristic_cache[state_key]\n",
    "        \n",
    "        h_value = self.problem.heuristic(state)\n",
    "        self.heuristic_cache[state_key] = h_value\n",
    "        return h_value\n",
    "    \n",
    "    def search(self, max_iterations=10000, time_limit=240, exact_days=True):\n",
    "        \"\"\"\n",
    "        Performs A* search to find an optimal training plan.\n",
    "        \n",
    "        Args:\n",
    "            max_iterations: Maximum number of iterations to prevent infinite loops\n",
    "            time_limit: Maximum time in seconds to run search\n",
    "            exact_days: Whether to require exact day match for goal state\n",
    "            \n",
    "        Returns:\n",
    "            The goal node if a solution is found, None otherwise\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize the start node\n",
    "        start_node = Node(self.problem.initial_state)\n",
    "        start_node.g = 0  # Cost from start to start is 0\n",
    "        start_node.h = self.get_heuristic(start_node.state)\n",
    "        start_node.f = start_node.g + start_node.h\n",
    "        \n",
    "        # Initialize open and closed sets\n",
    "        open_set = []  # Priority queue\n",
    "        heapq.heappush(open_set, (start_node.f, id(start_node), start_node))  # Using object ID as tiebreaker\n",
    "        \n",
    "        # Use a dictionary for O(1) lookups in the open set\n",
    "        open_dict = {self.get_state_key(start_node.state): start_node}\n",
    "        \n",
    "        # Closed set to track visited states\n",
    "        closed_set = set()\n",
    "        \n",
    "        iterations = 0\n",
    "        while open_set and iterations < max_iterations:\n",
    "            # Check time limit\n",
    "            if time.time() - start_time > time_limit:\n",
    "                print(f\"Search time limit reached ({time_limit}s)\")\n",
    "                break\n",
    "                \n",
    "            iterations += 1\n",
    "            \n",
    "            # Track max queue size for metrics\n",
    "            self.max_queue_size = max(self.max_queue_size, len(open_set))\n",
    "            \n",
    "            # Get node with lowest f-score\n",
    "            _, _, current_node = heapq.heappop(open_set)\n",
    "            current_key = self.get_state_key(current_node.state)\n",
    "            \n",
    "            # Remove from open dictionary\n",
    "            if current_key in open_dict:\n",
    "                del open_dict[current_key]\n",
    "            \n",
    "            # Check if we reached a goal state\n",
    "            day, fatigue, risk, performance, _ = current_node.state\n",
    "            \n",
    "            # Modified goal state check to enforce exact day match if requested\n",
    "            day_condition = (day == self.problem.target_day) if exact_days else (day >= self.problem.target_day)\n",
    "            \n",
    "            if (day_condition and \n",
    "                performance >= self.problem.target_perf and\n",
    "                fatigue <= self.problem.max_fatigue and\n",
    "                risk <= self.problem.max_risk):\n",
    "                self.execution_time = time.time() - start_time\n",
    "                print(f\"ML prediction cache: {self.prediction_hits} hits, {self.prediction_misses} misses\")\n",
    "                return current_node\n",
    "            \n",
    "            # Skip if we've already processed this state\n",
    "            if current_key in closed_set:\n",
    "                continue\n",
    "            \n",
    "            # Add to closed set\n",
    "            closed_set.add(current_key)\n",
    "            self.expanded_nodes += 1\n",
    "            \n",
    "            # Print progress periodically\n",
    "            if self.expanded_nodes % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"Expanded {self.expanded_nodes} nodes, queue size: {len(open_set)}, time: {elapsed:.1f}s\")\n",
    "            \n",
    "            # Get all possible actions for the current state\n",
    "            for action in self.problem.actions():\n",
    "                # Apply the action to get a new state\n",
    "                new_state = self.problem.apply_action(current_node.state, action)\n",
    "                \n",
    "                # Skip invalid states\n",
    "                if not self.problem.is_valid(new_state):\n",
    "                    continue\n",
    "                \n",
    "                # Create a new node for this state\n",
    "                new_node = Node(new_state, parent=current_node, action=action)\n",
    "                \n",
    "                # Calculate g-score (cost from start)\n",
    "                new_g = current_node.g + self.problem.cost(current_node.state, action)\n",
    "                \n",
    "                # Get the state key for checking in open/closed sets\n",
    "                new_key = self.get_state_key(new_state)\n",
    "                \n",
    "                # Skip if we've already processed this state\n",
    "                if new_key in closed_set:\n",
    "                    continue\n",
    "                \n",
    "                # If this state is already in the open set, check if our new path is better\n",
    "                if new_key in open_dict:\n",
    "                    existing_node = open_dict[new_key]\n",
    "                    if new_g >= existing_node.g:\n",
    "                        # Our path is not better, skip\n",
    "                        continue\n",
    "                \n",
    "                # This is a better path to the state, update or add to open set\n",
    "                new_node.g = new_g\n",
    "                new_node.h = self.get_heuristic(new_state)\n",
    "                new_node.f = new_node.g + new_node.h\n",
    "                \n",
    "                # Add to open set\n",
    "                heapq.heappush(open_set, (new_node.f, id(new_node), new_node))\n",
    "                open_dict[new_key] = new_node\n",
    "        \n",
    "        self.execution_time = time.time() - start_time\n",
    "        print(f\"ML prediction cache: {self.prediction_hits} hits, {self.prediction_misses} misses\")\n",
    "        \n",
    "        # If we've examined all nodes and haven't found a solution, return None\n",
    "        return None\n",
    "    \n",
    "    def reconstruct_path(self, goal_node, truncate_to_target_day=True):\n",
    "        \"\"\"\n",
    "        Reconstructs the path from start to goal node.\n",
    "        \n",
    "        Args:\n",
    "            goal_node: The final node to backtrack from\n",
    "            truncate_to_target_day: Whether to truncate the path to exactly target_day days\n",
    "            \n",
    "        Returns:\n",
    "            A list of actions (intensity, duration) from start to goal\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        current = goal_node\n",
    "        \n",
    "        # Collect all actions from goal to start\n",
    "        while current.parent:\n",
    "            actions.append(current.action)\n",
    "            current = current.parent\n",
    "        \n",
    "        # Reverse to get path from start to goal\n",
    "        actions = actions[::-1]\n",
    "        \n",
    "        # If truncating and we have more actions than target_day, trim the excess\n",
    "        if truncate_to_target_day and len(actions) > self.problem.target_day:\n",
    "            actions = actions[:self.problem.target_day]\n",
    "            print(f\"Note: Path truncated from {len(actions)} to {self.problem.target_day} days\")\n",
    "            \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb5c04",
   "metadata": {},
   "source": [
    "### 4.3. Local Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26eaff",
   "metadata": {},
   "source": [
    "#### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20eecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377aa0b",
   "metadata": {},
   "source": [
    "## 5. Constraints Satisfacton (CSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AthleteTrainingCSP:\n",
    "    \"\"\"\n",
    "    Implementation of a Constraint Satisfaction Problem approach for athlete training planning\n",
    "    using only backtracking search.\n",
    "    \n",
    "    The CSP approach models the athlete training planning problem with:\n",
    "    - Variables: Training activities for each day\n",
    "    - Domains: Possible training intensities and durations for each day\n",
    "    - Constraints: Fatigue limits and injury risk thresholds\n",
    "    - Objective: Maximize performance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "        initial_state=(0, 0.0, 0.0, 1.0),\n",
    "        target_day=30,\n",
    "        max_fatigue=2.7,\n",
    "        max_risk=0.5):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the CSP for athlete training planning.\n",
    "        \n",
    "        Args:\n",
    "            initial_state: Tuple of (day, fatigue, risk, performance)\n",
    "            target_day: Target training period length in days\n",
    "            max_fatigue: Maximum allowable fatigue level\n",
    "            max_risk: Maximum allowable injury risk\n",
    "        \"\"\"\n",
    "        \n",
    "        # an object of the problem definition\n",
    "        self.athlete_problem = AthletePerformanceProblem(initial_state=initial_state)\n",
    "        \n",
    "        # Thresholds\n",
    "        self.target_day = target_day\n",
    "        self.max_fatigue = max_fatigue\n",
    "        self.max_risk = max_risk\n",
    "        \n",
    "        # Initialize state with history\n",
    "        day, f, r, p = initial_state\n",
    "        self.initial_state = (day, f, r, p, [\n",
    "            {'load': 0.0,\n",
    "            'fatigue': f,\n",
    "            'injury_count': 0,\n",
    "            'days_since_game': 0,\n",
    "            'days_since_last_injury': 0}\n",
    "        ])\n",
    "        \n",
    "        self.intensities = [0.0, 0.3, 0.6, 0.9]\n",
    "        self.durations = [0, 60, 90, 120]\n",
    "        \n",
    "        # the domains: possible intensities and durations\n",
    "        self.domains = set()\n",
    "        \n",
    "        # all possible pairs of intensity and duration\n",
    "        for intensity in self.intensities:\n",
    "            for duration in self.durations:\n",
    "                if (intensity==0.0 and duration==0) or (intensity> 0.0 and duration>0):\n",
    "                    self.domains.add((intensity, duration))\n",
    "\n",
    "        self.transition_cache = {} # Cache for performance optimization\n",
    "        \n",
    "        # backtracking stats\n",
    "        self.backtrack_stats = {\n",
    "            'iterations': 0,\n",
    "            'max_depth': 0,\n",
    "            'pruning_count': 0\n",
    "        }\n",
    "\n",
    "    def get_domains(self):\n",
    "        return self.domains\n",
    "    \n",
    "    def apply_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: Tuple of (day, fatigue, risk, performance, history)\n",
    "            action: Tuple of (intensity, duration)\n",
    "            \n",
    "        Returns:\n",
    "            The resulting new state after applying the action\n",
    "        \"\"\"\n",
    "        \n",
    "        # caching to avoid recalculating\n",
    "        cache_key = (str(state), str(action))\n",
    "        if cache_key in self.transition_cache:\n",
    "            return self.transition_cache[cache_key]\n",
    "                \n",
    "        day, fatigue, risk, performance, history = state\n",
    "        \n",
    "        basic_state = (day, fatigue, risk, performance, history) # tuple of only four elements !!!\n",
    "        \n",
    "        # apply the transition\n",
    "        new_state = self.athlete_problem.apply_action(basic_state, action)\n",
    "        \n",
    "        self.transition_cache[cache_key] = new_state    # cache the answer\n",
    "        return new_state\n",
    "    \n",
    "    def check_constraints(self, state):\n",
    "        \"\"\"Check if the current state satisfies all constraints\"\"\"\n",
    "        _, fatigue, risk, _, _ = state\n",
    "        return fatigue <= self.max_fatigue and risk <= self.max_risk\n",
    "\n",
    "    def evaluate_solution(self, solution):\n",
    "        \"\"\"\n",
    "        Evaluate a complete training plan.\n",
    "        \n",
    "        Args:\n",
    "            solution: List of (intensity, duration) actions for each day\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        current_state = self.initial_state\n",
    "        states = [current_state]\n",
    "        \n",
    "        # Apply each action in the solution\n",
    "        for action in solution:\n",
    "            current_state = self.apply_action(current_state, action)\n",
    "            states.append(current_state)\n",
    "        \n",
    "        # Get final state\n",
    "        final_day, final_fatigue, final_risk, final_performance, _ = states[-1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        highest_fatigue = max(state[1] for state in states)\n",
    "        highest_risk = max(state[2] for state in states)\n",
    "        constraints_violated = any(state[1] > self.max_fatigue or state[2] > self.max_risk for state in states)\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        rest_days = sum(1 for action in solution if action[0] == 0.0 and action[1] == 0)\n",
    "        high_intensity_days = sum(1 for action in solution if action[0] >= 0.7)\n",
    "        total_workload = sum(self.athlete_problem.LOAD_PER_MIN.get(action[0], 0) * action[1] for action in solution)\n",
    "        \n",
    "        return {\n",
    "            'final_performance': final_performance,\n",
    "            'highest_fatigue': highest_fatigue,\n",
    "            'final_fatigue': final_fatigue,\n",
    "            'final_risk': final_risk,\n",
    "            'highest_risk': highest_risk,\n",
    "            'constraints_violated': constraints_violated,\n",
    "            'days_trained': final_day,\n",
    "            'rest_days': rest_days,\n",
    "            'high_intensity_days': high_intensity_days,\n",
    "            'total_workload': total_workload\n",
    "        }\n",
    "\n",
    "    def backtracking_search(self, time_limit=120):\n",
    "        \"\"\"\n",
    "        Backtracking search algorithm that stops after finding the first valid solution.\n",
    "        \n",
    "        Args:\n",
    "            time_limit: Maximum time in seconds to run the backtracking search\n",
    "            \n",
    "        Returns:\n",
    "            The first solution found that reaches the target day\n",
    "        \"\"\"\n",
    "        # reset our tracking stats for this search run\n",
    "        self.backtrack_stats = {\n",
    "            'iterations': 0,\n",
    "            'max_depth': 0,\n",
    "            'pruning_count': 0\n",
    "        }\n",
    "        \n",
    "        solution_found = False \n",
    "        solution = []   \n",
    "        \n",
    "        # nested backtracking function that does the recursive search\n",
    "        def _backtrack(assignment, current_state, depth=0):\n",
    "            \"\"\"\n",
    "            Recursive backtracking function that stops after finding the first valid solution.\n",
    "            \"\"\"\n",
    "            nonlocal solution_found, solution\n",
    "            if solution_found:\n",
    "                return True\n",
    "            \n",
    "            # track how many iterations we've done and how deep we've gone\n",
    "            self.backtrack_stats['iterations'] += 1\n",
    "            self.backtrack_stats['max_depth'] = max(self.backtrack_stats['max_depth'], depth)\n",
    "            \n",
    "            # make sure we haven't exceeded our time limit\n",
    "            if time.time() - start_time > time_limit:\n",
    "                return False\n",
    "            \n",
    "            day, fatigue, risk, performance, _ = current_state\n",
    "\n",
    "            # check if we've reached our goal - the target day\n",
    "            if day >= self.target_day:\n",
    "                # done, we assgined each day with of intensity and duration\n",
    "                solution_found = True\n",
    "                solution = assignment.copy()\n",
    "                return True\n",
    "            \n",
    "            # get possible actions for this day, ordered from most promising to least\n",
    "            actions = self._get_ordered_domain_values(current_state)\n",
    "            \n",
    "            for action in actions:\n",
    "                # try this action and see where it leads\n",
    "                new_state = self.apply_action(current_state, action)\n",
    "                \n",
    "                # make sure this action doesn't break any constraints\n",
    "                if self.check_constraints(new_state):\n",
    "                    assignment.append(action)\n",
    "                    \n",
    "                    # recursively continue down this path\n",
    "                    if _backtrack(assignment, new_state, depth + 1):\n",
    "                        return True  \n",
    "                    \n",
    "                    # remove the last added assignment and redo the backtracking\n",
    "                    assignment.pop()\n",
    "                    \n",
    "                    # if somehow a solution was found after backtracking, we can stop\n",
    "                    if solution_found:\n",
    "                        return True\n",
    "            \n",
    "            return False  # no solution found in this branch\n",
    "        \n",
    "        start_time = time.time()        \n",
    "        _backtrack([], self.initial_state)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if solution_found:\n",
    "            print(f\"First valid solution found in {end_time - start_time:.2f} seconds\")\n",
    "            # calculate how good this solution is\n",
    "            evaluation = self.evaluate_solution(solution)\n",
    "            print(f\"Performance: {evaluation['final_performance']:.2f}\")\n",
    "        else:\n",
    "            print(f\"No solution found within time limit of {time_limit} seconds\")\n",
    "        \n",
    "        return solution\n",
    "    \n",
    "    \n",
    "    #### FUNCTION DEFINITION: ORDERING DOMAIN VALUES\n",
    "    # This function is the heart of the CSP optimizer's decision-making process.\n",
    "    # It determines which training actions should be tried first during backtracking search.\n",
    "    def _get_ordered_domain_values(self, state):\n",
    "        \"\"\"\n",
    "        Advanced priority ordering of domain values (actions) with a sophisticated formula \n",
    "        that heavily prioritizes performance maximization above all else.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state which includes day, fatigue, risk, performance and history\n",
    "            \n",
    "        Returns:\n",
    "            List of actions ordered by most promising first (best actions at the beginning)\n",
    "        \"\"\"\n",
    "        # Unpacking the state variables for easier access\n",
    "        day, fatigue, risk, performance, history = state\n",
    "        actions = self.get_domains()\n",
    "        action_values = {}\n",
    "        \n",
    "        \n",
    "        #### CONFIGURATION: OPTIMIZATION WEIGHTS\n",
    "        PERFORMANCE_WEIGHT = 1000      # 1000 because its our primary objective\n",
    "        \n",
    "        # Headroom refers to the remaining capacity between the current fatigue/risk levels and their maximum allowable limits.\n",
    "        # It indicates how much more stress the athlete can safely handle before reaching their physiological limits.\n",
    "        # Higher headroom means more flexibility for intense training in the future.\n",
    "        FATIGUE_HEADROOM_WEIGHT = 15   \n",
    "        RISK_HEADROOM_WEIGHT = 20      \n",
    "        \n",
    "        ## Training pattern weights ensure physiologically sound training progression\n",
    "        RECOVERY_BONUS = 30            # Value the recovery when fatigue is high\n",
    "        LONG_TERM_POTENTIAL = 80      # Value actions with potential future payoff\n",
    "        \n",
    "        EFFICIENCY= 200\n",
    "        \n",
    "        #### PREPARATION: CALCULATE CURRENT STATE METRICS\n",
    "        ## headroom: how much more fatigue/risk the athlete can handle\n",
    "        # 0 means he can handle nothing\n",
    "        fatigue_headroom = max(0, self.max_fatigue - fatigue)\n",
    "        risk_headroom = max(0, self.max_risk - risk)\n",
    "        \n",
    "        # Count consecutive training days (days without rest)\n",
    "        days_since_rest = sum(1 for h in reversed(history) if h.get('load', 0) > 0.1)\n",
    "        \n",
    "        # fatigue and risk status\n",
    "        # >80% of maximum?  \n",
    "        high_fatigue = fatigue > (self.max_fatigue * 0.8)\n",
    "        high_risk = risk > (self.max_risk * 0.8)\n",
    "        remaining_days_factor= ((day+1)/self.target_day)\n",
    "        #### EVALUATION: evaluate EACH POSSIBLE ACTION\n",
    "        for action in actions:\n",
    "            intensity, duration = action\n",
    "            \n",
    "            future_state = self.apply_action(state, action)\n",
    "            _, future_fatigue, future_risk, future_performance, _ = future_state\n",
    "            \n",
    "            ## violate constraints are given negative infinity (so it goes to the end when the domain is sorted)\n",
    "            if future_fatigue > self.max_fatigue or future_risk > self.max_risk:\n",
    "                action_values[action] = -float('inf') \n",
    "                self.backtrack_stats['pruning_count'] += 1 \n",
    "                continue  \n",
    "            \n",
    "            perf_improvement = future_performance - performance\n",
    "            \n",
    "            ## Training efficiency metrics\n",
    "            # Calculate standardized training load (hours equivalent)\n",
    "            training_load = intensity * (duration / 60) \n",
    "\n",
    "            # Calculate performance gain per unit of training load (with safeguard against division by zero)\n",
    "            efficiency = perf_improvement / max(0.5, training_load) if training_load > 0 else 0\n",
    "            \n",
    "            # Calculate future headroom values to see how action affects future training capacity\n",
    "            future_fatigue_headroom = max(0, self.max_fatigue - future_fatigue)\n",
    "            future_risk_headroom = max(0, self.max_risk - future_risk)\n",
    "                        \n",
    "            recovery_value = 0\n",
    "            if intensity == 0 and duration == 0:  # Rest day\n",
    "                if high_fatigue:\n",
    "                    # we give bonuses for the rest if the athlete have high fatigue \n",
    "                    recovery_value = fatigue * RECOVERY_BONUS\n",
    "                \n",
    "                if days_since_rest > 3:  \n",
    "                    # bonus if the athlete hasn't rested in >3 days\n",
    "                    recovery_value += days_since_rest * 5\n",
    "            \n",
    "            \n",
    "            # this one has the most important impact on finding an optimal schudule            \n",
    "            # high intensity training -> future capacity\n",
    "            # we then multiply by the factor of the remaining days because we want the performance to be maximized at the target day not before\n",
    "            # this factor may be negative because remaining_days_factor\n",
    "            long_term_value = (intensity)*(duration / 60)* (1-remaining_days_factor) \n",
    "\n",
    "\n",
    "            ## Performance valuation with exponential weighting\n",
    "            # Square positive performance improvements to emphasize their value\n",
    "            # Linear weighting for negative performance (avoids excessively punishing small negatives)\n",
    "            performance_value = perf_improvement ** 2 * PERFORMANCE_WEIGHT if perf_improvement > 0 else perf_improvement * PERFORMANCE_WEIGHT\n",
    "            \n",
    "            #print(f\"day:{day}\")\n",
    "            #print(f\"long_term_value: {long_term_value}\")\n",
    "            #print(f\"prformance: {performance_value}\")\n",
    "            #### FINAL SCORING: COMBINE ALL COMPONENTS\n",
    "            action_values[action] = (\n",
    "                performance_value*remaining_days_factor +\n",
    "                efficiency * EFFICIENCY + \n",
    "                (future_fatigue_headroom - fatigue_headroom) * FATIGUE_HEADROOM_WEIGHT + \n",
    "                (future_risk_headroom - risk_headroom) * RISK_HEADROOM_WEIGHT +\n",
    "                recovery_value + \n",
    "                long_term_value * LONG_TERM_POTENTIAL \n",
    "            )\n",
    "        # Sort actions in descending order (best first) based on the scoring system we set\n",
    "        return sorted(actions, key=lambda a: action_values.get(a, 0), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d4c7a",
   "metadata": {},
   "source": [
    "## 6. Test Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a63332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_plan(problem, initial_state, path, additional_info=None):\n",
    "    \"\"\"\n",
    "    Display the training plan as a table.\n",
    "    \n",
    "    Args:\n",
    "        problem: The problem instance\n",
    "        initial_state: The initial state of the problem\n",
    "        path: List of actions forming the solution path\n",
    "        additional_info: Dictionary with additional information to display\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining Plan:\")\n",
    "    print(\"Day | Intensity | Duration | Fatigue | Risk | Performance\")\n",
    "    print(\"----|-----------|----------|---------|------|------------\")\n",
    "    \n",
    "    # Display initial state\n",
    "    state = initial_state\n",
    "    day = state[0]\n",
    "    print(f\"{day:3d} | {'-':9} | {'-':8} |  {state[1]:.2f}   | {state[2]:.2f} | {state[3]:.2f}\")\n",
    "    \n",
    "    # Display each day in the training plan\n",
    "    current_state = state\n",
    "    for action in path:\n",
    "        # Apply action to get the new state\n",
    "        current_state = problem.apply_action(current_state, action)\n",
    "        day, fatigue, risk, performance, _ = current_state\n",
    "        intensity, duration = action\n",
    "        \n",
    "        # Categorize rest days based on intensity\n",
    "        if intensity == 0.0 and duration == 0.0:\n",
    "            if fatigue > 2.0:\n",
    "                rest_type = \"Passive Rest\"\n",
    "            else:\n",
    "                rest_type = \"Active Rest\"\n",
    "            print(f\"{day:3d} | {rest_type:9} | {'-':8} |  {fatigue:.2f}   | {risk:.2f} | {performance:.2f}\")\n",
    "        else:\n",
    "            print(f\"{day:3d} | {intensity:9.1f} | {duration:8.1f} |  {fatigue:.2f}   | {risk:.2f} | {performance:.2f}\")\n",
    "    \n",
    "    # Display final state summary\n",
    "    final_day, final_fatigue, final_risk, final_perf, _ = current_state\n",
    "    print(\"\\nFinal State:\")\n",
    "    print(f\"Day: {final_day}\")\n",
    "    print(f\"Fatigue: {final_fatigue:.2f}/5.00\")\n",
    "    print(f\"Risk: {final_risk:.2f}/1.00\")\n",
    "    print(f\"Performance: {final_perf:.2f}/10.00\")\n",
    "    \n",
    "    # Display additional information if provided\n",
    "    if additional_info:\n",
    "        print(\"\\nSearch Statistics:\")\n",
    "        for key, value in additional_info.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{key}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Report whether goal was achieved\n",
    "    if hasattr(problem, 'target_day') and hasattr(problem, 'target_perf'):\n",
    "        if final_day >= problem.target_day and final_perf >= problem.target_perf:\n",
    "            print(\"\\nGoal achieved!\")\n",
    "        else:\n",
    "            print(\"\\nGoal not achieved.\")\n",
    "\n",
    "def test_bfs_search(initial_state, target_day, target_perf, max_fatigue, max_risk):\n",
    "    \"\"\"\n",
    "    Test the BFS algorithm with given parameters.\n",
    "    \"\"\"\n",
    "    print(\"Testing BFS Algorithm\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    # Create the athlete performance problem with parameters\n",
    "    problem = AthletePerformanceProblem(initial_state=initial_state)\n",
    "    problem.target_day = target_day\n",
    "    problem.target_perf = target_perf\n",
    "    problem.max_fatigue = max_fatigue\n",
    "    problem.max_risk = max_risk\n",
    "    \n",
    "    # Create the BFS algorithm\n",
    "    searcher = BFSSearch(problem)\n",
    "\n",
    "    # Run the search algorithm\n",
    "    print(\"Starting search...\")\n",
    "    start_time = time.time()\n",
    "    goal_node = searcher.search()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Search completed. Nodes explored: {searcher.expanded_nodes}\")\n",
    "    \n",
    "    # Report the results\n",
    "    if goal_node is None:\n",
    "        print(\"No solution found.\")\n",
    "    else:\n",
    "        # Reconstruct the path from initial state to goal\n",
    "        path = searcher.reconstruct_path(goal_node)\n",
    "        \n",
    "        # Additional information for display\n",
    "        additional_info = {\n",
    "            \"Execution Time\": execution_time,\n",
    "            \"Expanded Nodes\": searcher.expanded_nodes,\n",
    "            \"Max Queue Size\": getattr(searcher, 'max_queue_size', 'N/A')\n",
    "        }\n",
    "        \n",
    "        # Display the training plan\n",
    "        display_training_plan(problem, problem.initial_state, path, additional_info)\n",
    "\n",
    "def test_dfs_search(initial_state, target_day, target_perf, max_fatigue, max_risk):\n",
    "    \"\"\"\n",
    "    Test the DFS algorithm with given parameters.\n",
    "    \"\"\"\n",
    "    print(\"Testing DFS Algorithm\")\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    # Create the athlete performance problem with parameters\n",
    "    problem = AthletePerformanceProblem(initial_state=initial_state)\n",
    "    problem.target_day = target_day\n",
    "    problem.target_perf = target_perf\n",
    "    problem.max_fatigue = max_fatigue\n",
    "    problem.max_risk = max_risk\n",
    "\n",
    "    # Create the DFS searcher\n",
    "    searcher = DFSSearch(problem)\n",
    "\n",
    "    # Run the search algorithm\n",
    "    print(\"Starting search...\")\n",
    "    start_time = time.time()\n",
    "    goal_node = searcher.search()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Search completed. Nodes explored: {searcher.expanded_nodes}\")\n",
    "\n",
    "    if goal_node is None:\n",
    "        print(\"No solution found.\")\n",
    "    else:\n",
    "        # Reconstruct the path from initial state to goal\n",
    "        path = searcher.reconstruct_path(goal_node)\n",
    "        \n",
    "        # Additional information for display\n",
    "        additional_info = {\n",
    "            \"Execution Time\": execution_time,\n",
    "            \"Expanded Nodes\": searcher.expanded_nodes,\n",
    "            \"Max Queue Size\": getattr(searcher, 'max_queue_size', 'N/A')\n",
    "        }\n",
    "        \n",
    "        # Display the training plan\n",
    "        display_training_plan(problem, problem.initial_state, path, additional_info)\n",
    "\n",
    "def test_ucs_search(initial_state, target_day, target_perf, max_fatigue, max_risk):\n",
    "    \"\"\"\n",
    "    Test the UCS algorithm with given parameters.\n",
    "    \"\"\"\n",
    "    print(\"Testing Uniform Cost Search Algorithm\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    # Create the athlete performance problem with parameters\n",
    "    problem = AthletePerformanceProblem(initial_state=initial_state)\n",
    "    problem.target_day = target_day\n",
    "    problem.target_perf = target_perf\n",
    "    problem.max_fatigue = max_fatigue\n",
    "    problem.max_risk = max_risk\n",
    "    \n",
    "    # Create the UCS searcher\n",
    "    searcher = UCSSearch(problem)\n",
    "    \n",
    "    # Run the search algorithm\n",
    "    print(\"Starting search...\")\n",
    "    goal_node = searcher.search()\n",
    "    print(f\"Search completed. Nodes explored: {searcher.expanded_nodes}\")\n",
    "    \n",
    "    if goal_node is None:\n",
    "        print(\"No solution found.\")\n",
    "    else:\n",
    "        path = searcher.reconstruct_path(goal_node)\n",
    "        \n",
    "        # Additional information for display\n",
    "        additional_info = {\n",
    "            \"Total Path Cost\": goal_node.g,\n",
    "            \"Execution Time\": searcher.execution_time,\n",
    "            \"Expanded Nodes\": searcher.expanded_nodes,\n",
    "            \"Max Queue Size\": searcher.max_queue_size\n",
    "        }\n",
    "        \n",
    "        # Display the training plan\n",
    "        display_training_plan(problem, problem.initial_state, path, additional_info)\n",
    "\n",
    "def test_greedy_search(initial_state, target_day, target_perf, max_fatigue, max_risk):\n",
    "    \"\"\"\n",
    "    Test the greedy search algorithm with given parameters.\n",
    "    \"\"\"\n",
    "    print(\"Testing Greedy Search Algorithm\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    # Create the athlete performance problem with parameters\n",
    "    problem = AthletePerformanceProblem(initial_state=initial_state)\n",
    "    problem.target_day = target_day\n",
    "    problem.target_perf = target_perf\n",
    "    problem.max_fatigue = max_fatigue\n",
    "    problem.max_risk = max_risk\n",
    "    \n",
    "    # Create the greedy searcher\n",
    "    searcher = GreedySearch(problem)\n",
    "    \n",
    "    # Run the search algorithm\n",
    "    print(\"Starting search...\")\n",
    "    goal_node = searcher.search()\n",
    "    print(f\"Search completed. Nodes explored: {searcher.expanded_nodes}\")\n",
    "    \n",
    "    if goal_node is None:\n",
    "        print(\"No solution found.\")\n",
    "    else:\n",
    "        path = searcher.reconstruct_path(goal_node)\n",
    "        \n",
    "        # Additional information for display\n",
    "        additional_info = {\n",
    "            \"Execution Time\": searcher.execution_time,\n",
    "            \"Expanded Nodes\": searcher.expanded_nodes,\n",
    "            \"Max Queue Size\": searcher.max_queue_size\n",
    "        }\n",
    "        \n",
    "        # Display the training plan\n",
    "        display_training_plan(problem, problem.initial_state, path, additional_info)\n",
    "\n",
    "def test_astar_search(initial_state, target_day, target_perf, max_fatigue, max_risk, max_iterations, time_limit, exact_days):\n",
    "    \"\"\"\n",
    "    Test the A* search algorithm with given parameters.\n",
    "    \"\"\"\n",
    "    print(f\"Testing A* Search Algorithm\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    # Create the problem with specified parameters\n",
    "    problem = AthletePerformanceProblem(initial_state=initial_state)\n",
    "    problem.target_day = target_day\n",
    "    problem.target_perf = target_perf\n",
    "    problem.max_fatigue = max_fatigue\n",
    "    problem.max_risk = max_risk\n",
    "    \n",
    "    # Create and run the A* search\n",
    "    astar = AStarSearch(problem)\n",
    "    goal_node = astar.search(max_iterations=max_iterations, time_limit=time_limit, exact_days=exact_days)\n",
    "    \n",
    "    if goal_node:\n",
    "        # Get the path of actions with truncation if exact_days is True\n",
    "        actions = astar.reconstruct_path(goal_node, truncate_to_target_day=exact_days)\n",
    "        \n",
    "        # Additional information for display\n",
    "        additional_info = {\n",
    "            \"Execution Time\": astar.execution_time,\n",
    "            \"Expanded Nodes\": astar.expanded_nodes,\n",
    "            \"Max Queue Size\": astar.max_queue_size,\n",
    "            \"ML Prediction Cache Hits\": astar.prediction_hits if hasattr(astar, 'prediction_hits') else 'N/A',\n",
    "            \"ML Prediction Cache Misses\": astar.prediction_misses if hasattr(astar, 'prediction_misses') else 'N/A'\n",
    "        }\n",
    "        \n",
    "        # Display the training plan\n",
    "        display_training_plan(problem, problem.initial_state, actions, additional_info)\n",
    "    else:\n",
    "        print(\"\\nNo solution found within iteration/time limit.\")\n",
    "        print(f\"Nodes expanded: {astar.expanded_nodes}\")\n",
    "        print(f\"Max queue size: {astar.max_queue_size}\")\n",
    "        print(f\"Execution time: {astar.execution_time:.2f} seconds\")\n",
    "        if hasattr(astar, 'prediction_hits'):\n",
    "            print(f\"ML prediction cache: {astar.prediction_hits} hits, {astar.prediction_misses} misses\")\n",
    "\n",
    "def test_backtracking_csp_max_performance(initial_state, target_day, max_fatigue, max_risk, time_limit):\n",
    "    \"\"\"\n",
    "    Test the CSP approach with optimized backtracking for maximum performance.\n",
    "    \"\"\"\n",
    "    print(\"Testing CSP Athlete Training Planner for Maximum Performance\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    \n",
    "    # CSP problem with given parameters\n",
    "    problem = AthleteTrainingCSP(\n",
    "        initial_state=initial_state,\n",
    "        target_day=target_day,\n",
    "        max_fatigue=max_fatigue,\n",
    "        max_risk=max_risk\n",
    "    )\n",
    "    \n",
    "    print(\"Finding solution to maximize performance...\")\n",
    "    start_time = time.time()\n",
    "    solution = problem.backtracking_search(time_limit=time_limit) \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Search completed in {execution_time:.2f} seconds\")\n",
    "    print(f\"Backtracking iterations: {problem.backtrack_stats['iterations']}\")\n",
    "    print(f\"Maximum depth reached: {problem.backtrack_stats['max_depth']}\")\n",
    "    print(f\"Number of branches pruned: {problem.backtrack_stats['pruning_count']}\")\n",
    "    \n",
    "    if solution is None:\n",
    "        print(\"No solution found.\")\n",
    "        return\n",
    "    \n",
    "    # Additional information for display\n",
    "    additional_info = {\n",
    "        \"Execution Time\": execution_time,\n",
    "        \"Backtracking Iterations\": problem.backtrack_stats['iterations'],\n",
    "        \"Maximum Depth\": problem.backtrack_stats['max_depth'],\n",
    "        \"Branches Pruned\": problem.backtrack_stats['pruning_count']\n",
    "    }\n",
    "    \n",
    "    # Display the training plan\n",
    "    display_training_plan(problem, problem.initial_state, solution, additional_info)\n",
    "    \n",
    "    # Evaluate solution\n",
    "    evaluation = problem.evaluate_solution(solution)\n",
    "    print(\"\\nSolution Evaluation:\")\n",
    "    print(f\"Final Performance: {evaluation['final_performance']:.2f}\")\n",
    "    print(f\"Constraints Violated: {'Yes' if evaluation['constraints_violated'] else 'No'}\")\n",
    "    print(f\"Highest Fatigue: {evaluation['highest_fatigue']:.2f}/{problem.max_fatigue:.2f}\")\n",
    "    print(f\"Highest Risk: {evaluation['highest_risk']:.2f}/{problem.max_risk:.2f}\")\n",
    "    print(f\"Rest Days: {evaluation['rest_days']}/{evaluation['days_trained']}\")\n",
    "    print(f\"High Intensity Days: {evaluation['high_intensity_days']}\")\n",
    "    print(f\"Total Workload: {evaluation['total_workload']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9033738",
   "metadata": {},
   "source": [
    "## 7. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the parameters\n",
    "INIT_PERFORMANCE = 6.5\n",
    "INIT_FATIGUE = 1.5\n",
    "INIT_RISK = 0.15\n",
    "\n",
    "TARGET_PERFORMANCE = 7\n",
    "TRAINING_DAYS = 10\n",
    "MAX_FATIGUE = 2.65\n",
    "MAX_RISK = 0.2\n",
    "\n",
    "# A start specific\n",
    "TIME_LIMIT= 600\n",
    "MAX_ITER= 50000\n",
    "EXACT_DAYS= True\n",
    "\n",
    "\n",
    "initial_state = (0, INIT_FATIGUE, INIT_RISK, INIT_PERFORMANCE)\n",
    "\n",
    "def test_all():\n",
    "\n",
    "    test_greedy_search(initial_state=initial_state, target_day=TRAINING_DAYS, target_perf=TARGET_PERFORMANCE, max_fatigue=MAX_FATIGUE, max_risk=MAX_RISK)\n",
    "\n",
    "    test_backtracking_csp_max_performance(initial_state=initial_state, target_day=TRAINING_DAYS, max_fatigue=MAX_FATIGUE, max_risk=MAX_RISK, time_limit=TIME_LIMIT)\n",
    "\n",
    "    test_astar_search(initial_state=initial_state, target_day=TRAINING_DAYS, target_perf=TARGET_PERFORMANCE, max_fatigue=MAX_FATIGUE, max_risk=MAX_RISK, max_iterations=MAX_ITER, time_limit=TIME_LIMIT, exact_days=EXACT_DAYS)\n",
    "    \n",
    "    test_bfs_search(initial_state=initial_state, target_day=TRAINING_DAYS, target_perf=TARGET_PERFORMANCE, max_fatigue=MAX_FATIGUE, max_risk=MAX_RISK)\n",
    "\n",
    "    test_dfs_search(initial_state=initial_state, target_day=TRAINING_DAYS, target_perf=TARGET_PERFORMANCE, max_fatigue=MAX_FATIGUE, max_risk=MAX_RISK)\n",
    "\n",
    "    test_ucs_search(initial_state=initial_state, target_day=TRAINING_DAYS, target_perf=TARGET_PERFORMANCE, max_fatigue=MAX_FATIGUE, max_risk=MAX_RISK)\n",
    "\n",
    "\n",
    "test_all()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
