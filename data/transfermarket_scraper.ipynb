{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Gregor Kobel -> https://www.transfermarkt.com/gregor-kobel/profil/spieler/257814\n",
      "Scraping Marcel Lotka -> https://www.transfermarkt.com/marcel-lotka/profil/spieler/453737\n",
      "Scraping Alexander Meyer -> https://www.transfermarkt.com/alexander-meyer/profil/spieler/76158\n",
      "Scraping Nico Schlotterbeck -> https://www.transfermarkt.com/nico-schlotterbeck/profil/spieler/388198\n",
      "Scraping Niklas Süle -> https://www.transfermarkt.com/niklas-sule/profil/spieler/166601\n",
      "Scraping Mats Hummels -> https://www.transfermarkt.com/mats-hummels/profil/spieler/39728\n",
      "Scraping Hendry Blank -> https://www.transfermarkt.com/hendry-blank/profil/spieler/804831\n",
      "Scraping Antonios Papadopoulos -> https://www.transfermarkt.com/antonios-papadopoulos/profil/spieler/482573\n",
      "Scraping Ian Maatsen -> https://www.transfermarkt.com/ian-maatsen/profil/spieler/485585\n",
      "Scraping Ramy Bensebaini -> https://www.transfermarkt.com/ramy-bensebaini/profil/spieler/284732\n",
      "Scraping Guille Bueno -> https://www.transfermarkt.com/guille-bueno/profil/spieler/854079\n",
      "Scraping Julian Ryerson -> https://www.transfermarkt.com/julian-ryerson/profil/spieler/370789\n",
      "Scraping Marius Wolf -> https://www.transfermarkt.com/marius-wolf/profil/spieler/193900\n",
      "Scraping Thomas Meunier -> https://www.transfermarkt.com/thomas-meunier/profil/spieler/100986\n",
      "Scraping Mateu Morey -> https://www.transfermarkt.com/mateu-morey/profil/spieler/388513\n",
      "Scraping Salih Özcan -> https://www.transfermarkt.com/salih-ozcan/profil/spieler/244940\n",
      "Scraping Emre Can -> https://www.transfermarkt.com/emre-can/profil/spieler/119296\n",
      "Scraping Abdoulaye Kamara -> https://www.transfermarkt.com/abdoulaye-kamara/profil/spieler/718232\n",
      "Scraping Felix Nmecha -> https://www.transfermarkt.com/felix-nmecha/profil/spieler/406640\n",
      "Scraping Marcel Sabitzer -> https://www.transfermarkt.com/marcel-sabitzer/profil/spieler/106987\n",
      "Scraping Kjell Wätjen -> https://www.transfermarkt.com/kjell-watjen/profil/spieler/884810\n",
      "Scraping Ole Pohlmann -> https://www.transfermarkt.com/ole-pohlmann/profil/spieler/405690\n",
      "Scraping Julian Brandt -> https://www.transfermarkt.com/julian-brandt/profil/spieler/187492\n",
      "Scraping Giovanni Reyna -> https://www.transfermarkt.com/giovanni-reyna/profil/spieler/504215\n",
      "Scraping Marco Reus -> https://www.transfermarkt.com/marco-reus/profil/spieler/35207\n",
      "Scraping Jadon Sancho -> https://www.transfermarkt.com/jadon-sancho/profil/spieler/401173\n",
      "Scraping Jamie Gittens -> https://www.transfermarkt.com/jamie-gittens/profil/spieler/670882\n",
      "Scraping Julien Duranville -> https://www.transfermarkt.com/julien-duranville/profil/spieler/819215\n",
      "Scraping Thorgan Hazard -> https://www.transfermarkt.com/thorgan-hazard/profil/spieler/102226\n",
      "Scraping Donyell Malen -> https://www.transfermarkt.com/donyell-malen/profil/spieler/326029\n",
      "Scraping Karim Adeyemi -> https://www.transfermarkt.com/karim-adeyemi/profil/spieler/496094\n",
      "Scraping Samuel Bamba -> https://www.transfermarkt.com/samuel-bamba/profil/spieler/684318\n",
      "Scraping Youssoufa Moukoko -> https://www.transfermarkt.com/youssoufa-moukoko/profil/spieler/467720\n",
      "Scraping Niclas Füllkrug -> https://www.transfermarkt.com/niclas-fullkrug/profil/spieler/75489\n",
      "Scraping Sébastien Haller -> https://www.transfermarkt.com/sebastien-haller/profil/spieler/181375\n",
      "Scraping Paris Brunner -> https://www.transfermarkt.com/paris-brunner/profil/spieler/918414\n",
      "Scraping completed! Data saved to players_with_injuries.json.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "base_url = \"https://www.transfermarkt.com\"\n",
    "squad_url = \"https://www.transfermarkt.com/borussia-dortmund/kader/verein/16/saison_id/2023/plus/1\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(squad_url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "players_data = []\n",
    "\n",
    "# Get all player profile links\n",
    "for row in soup.select(\"table.items > tbody > tr\"):\n",
    "    if \"class\" in row.attrs and \"odd\" not in row[\"class\"] and \"even\" not in row[\"class\"]:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Basic info\n",
    "        name_tag = row.find(\"td\", class_=\"hauptlink\").a\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        profile_path = name_tag['href']\n",
    "        profile_url = base_url + profile_path\n",
    "\n",
    "        print(f\"Scraping {name} -> {profile_url}\")\n",
    "        profile_res = requests.get(profile_url, headers=headers)\n",
    "        profile_soup = BeautifulSoup(profile_res.content, \"html.parser\")\n",
    "\n",
    "        # Extract age, height, weight, position\n",
    "        info_box = profile_soup.find(\"div\", class_=\"info-table\")\n",
    "        facts = profile_soup.select(\"div.info-table > div\")\n",
    "\n",
    "        age = height = weight = position = \"N/A\"\n",
    "\n",
    "        for fact in facts:\n",
    "            label = fact.find(\"span\", class_=\"info-label\")\n",
    "            if not label:\n",
    "                continue\n",
    "            key = label.get_text(strip=True)\n",
    "            value = fact.find(\"span\", class_=\"info-content\").get_text(strip=True)\n",
    "\n",
    "            if \"Age\" in key:\n",
    "                age = value\n",
    "            elif \"Height\" in key:\n",
    "                height = value\n",
    "            elif \"Weight\" in key:\n",
    "                weight = value\n",
    "            elif \"Position\" in key:\n",
    "                position = value\n",
    "\n",
    "        # Look for injury table (if any)\n",
    "        injury_url = profile_url.replace(\"profil\", \"verletzungen\")  # /profil/ → /verletzungen/\n",
    "        injury_res = requests.get(injury_url, headers=headers)\n",
    "        injury_soup = BeautifulSoup(injury_res.content, \"html.parser\")\n",
    "\n",
    "        injuries = []\n",
    "        injury_table = injury_soup.find(\"table\", class_=\"items\")\n",
    "        if injury_table:\n",
    "            for injury_row in injury_table.select(\"tbody > tr\"):\n",
    "                cols = injury_row.find_all(\"td\")\n",
    "                if len(cols) >= 5:\n",
    "                    injury_type = cols[1].get_text(strip=True)\n",
    "                    start_date = cols[2].get_text(strip=True)\n",
    "                    end_date = cols[3].get_text(strip=True)\n",
    "                    days_missed = cols[4].get_text(strip=True)\n",
    "\n",
    "                    injuries.append({\n",
    "                        \"type\": injury_type,\n",
    "                        \"start\": start_date,\n",
    "                        \"end\": end_date,\n",
    "                        \"days\": days_missed\n",
    "                    })\n",
    "\n",
    "        players_data.append({\n",
    "            \"Name\": name,\n",
    "            \"Age\": age,\n",
    "            \"Height\": height,\n",
    "            \"Weight\": weight,\n",
    "            \"Position\": position,\n",
    "            \"Injuries\": injuries\n",
    "        })\n",
    "\n",
    "        time.sleep(random.uniform(2, 4))  # Be nice to the server\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with player: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(players_data)\n",
    "df.to_json(\"players_with_injuries.json\", orient=\"records\", indent=2)\n",
    "\n",
    "print(\"Scraping completed! Data saved to players_with_injuries.json.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
